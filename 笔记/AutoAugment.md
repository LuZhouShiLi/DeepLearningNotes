# AutoAugment:Learning Augmentation Strategies from Data

## 摘要

🔬 研究方法: 本文描述了一种名为AutoAugment的简单程序，通过这个程序可以自动寻找改进的数据增强策略。研究设计了一个策略空间，其中策略包含多个子策略，在每个小批量数据中针对每张图片随机选择一个子策略。每个子策略由两个操作组成，每个操作是图像处理函数（如平移、旋转或剪切），以及应用这些函数的概率和强度。作者使用搜索算法寻找最佳策略，以便在目标数据集上获得神经网络的最高验证准确度。
📈 结果和发现: 该方法在CIFAR-10、CIFAR-100、SVHN和ImageNet数据集上达到了最先进的准确度，且无需额外数据。在ImageNet上，Top-1准确率达到了83.5%，比先前的纪录高出0.4%。在CIFAR-10上，错误率降低到1.5%，比之前的最佳成绩低0.6%。此外，发现的增强策略在不同数据集之间是可迁移的，例如，在ImageNet学习到的策略可以很好地迁移到其他数据集上并取得显著改进，如Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, 和 Stanford Cars。
🔄 对比学习/时序数据/信号处理分类对此研究的影响: 这项研究不专门涉及对比学习、时序数据或信号处理分类，而是关注自动化搜索数据增强策略来提高图像分类器的准确性。不过，找到的最优数据增强策略对于提高这些领域内神经网络模型的性能也可能有借鉴意义。

## Introduction

* 这段内容介绍了深度神经网络在处理大量数据时的强大性能，以及数据增强作为提高数据量和多样性的有效技术。数据增强通过随机方式增加数据的变化，常见于图像领域，例如通过平移几个像素或水平翻转图像。直观上，数据增强是用来教导模型识别数据域中的不变性：例如，对象的分类常常对水平翻转或平移不敏感。网络架构也可以用来固化不变性，例如卷积网络内置了平移不变性。然而，使用数据增强来纳入潜在的不变性可能比直接在模型架构中硬编码这些不变性来得更加简单。这意味着通过数据增强，我们可以更灵活地扩充模型对不同变化的适应性，而不必对网络架构进行复杂的定制。

* 这段内容扩展了之前关于数据增强的讨论，指出机器学习和计算机视觉社区的主要关注点之一是改进网络架构，而寻找能够纳入更多不变性的更好的数据增强方法却没有得到足够的重视。例如，在ImageNet上，自2012年引入的数据增强方法至今仍然是标准做法，仅有细微的变化。即使在特定数据集上找到了增强改进，它们也往往无法有效地迁移到其他数据集上。
* 本文的目标是自动化寻找目标数据集有效的数据增强策略的过程。具体来说，每个策略表达了多种可能的增强操作的选择和顺序，每个操作是一个图像处理功能（例如，平移、旋转或颜色标准化），以及应用该功能的概率和强度。研究者们使用搜索算法来找到最佳的操作选择和顺序，以便训练出在验证精度上最好的神经网络。在实验中，使用了强化学习作为搜索算法，但作者相信如果使用更好的算法，结果可以进一步改进。
* 实验表明，AutoAugment 在以下两种情况下取得了优秀的效果：1) 直接在感兴趣的数据集上应用 AutoAugment 来发现最佳的增强策略（AutoAugment-direct），2) 学到的策略可以迁移到新的数据集上（AutoAugment-transfer）。首先，就直接应用而言，该方法在 CIFAR-10、简化版 CIFAR-10、CIFAR-100、SVHN、简化版 SVHN 和 ImageNet（无额外数据）等数据集上实现了最先进的准确率。例如，在 CIFAR-10 上，错误率降至1.5%，比之前最先进的成绩低了0.6%。在 SVHN 上，将最先进的错误率从1.3%降低到1.0%。在简化的数据集上，该方法实现了与半监督方法相媲美的性能，而无需任何未标记数据。在 ImageNet 上，Top-1 准确率提高到了83.5%，比之前的纪录高0.4%。其次，如果直接应用代价过高，则转移增强策略可以是一个很好的替代方案。在转移增强策略方面，我们展示了在一个任务上发现的策略可以很好地推广到不同的模型和数据集。例如，在 ImageNet 上发现的策略在多个 FGVC 数据集上取得了显著的改进。即使在对于 ImageNet 预训练权重微调没有明显帮助的数据集上[26]，如 Stanford Cars [27]和 FGVC Aircraft [38]，使用 ImageNet 策略训练将测试集误差分别降低了1.2%和1.8%。这一结果表明，转移数据增强策略提供了一种与标准权重转移学习不同的方法。论文的第一部分的表1中总结了结果。


## 2. Related Work

* 这一段强调了传统的图像识别领域中数据增强方法很多都是手工设计的，并且最佳的数据增强策略是特定于数据集的。比如，在MNIST数据集中，许多排名靠前的模型使用了弹性变形、缩放、平移和旋转这些增强方法。而在自然图像数据集，如CIFAR-10和ImageNet，更常使用随机裁剪、图像镜像和颜色变换/白化等方法。由于这些方法是手工设计的，所以需要专家知识和时间成本。而作者提出的从数据中学习数据增强策略的方法理论上可以用于任何数据集，而不仅仅是一个特定的数据集。这表明作者旨在提出一种更加普适和自动化的数据增强方法，不再依赖于专家先验知识和大量的手动实验。

* 本段内容介绍了一种自动化的方法来从数据中找到数据增强策略。灵感来自于最近在架构搜索领域的进步，其中利用了强化学习和进化方法从数据中发现模型架构。尽管这些方法改进了人工设计的架构，使用架构搜索单独并未能突破CIFAR-10上2%的错误率屏障。
* 之前学习数据增强的尝试包括Smart Augmentation，它提出了一个自动通过合并同一类别的两个或更多样本来生成增强数据的网络。Tran等人使用了基于从训练集学到的分布生成数据的贝叶斯方法。DeVries和Taylor在学到的特征空间中使用简单变换来增强数据。
* 生成对抗网络（GAN）也被用于生成额外的数据。我们的方法与生成模型的主要区别在于，我们的方法生成的是符号变换操作，而生成模型（如GAN）直接生成增强的数据。Ratner等人的研究则是个例外，他们使用GAN来生成描述数据增强策略的序列。

## 3. AutoAugment: Searching for best Augmentation policies Directly on the Dataset of Interest

* 此段落阐述了研究者如何将寻找最佳数据增强策略的问题形式化为一个离散搜索问题，并概述了他们的方法包含两个组件：一个搜索算法和一个搜索空间。简而言之，搜索算法（实现为控制器RNN）会抽取一个数据增强策略S，该策略包含了关于使用哪种图像处理操作、每批使用该操作的概率以及该操作的幅度的信息。该方法的关键在于策略S将被用来训练一个固定架构的神经网络，其验证精度R将反馈给控制器以便更新。由于R不是可微的，控制器将通过策略梯度方法更新。以下部分将详细描述这两个组件。
* 搜索空间细节：在他们的搜索空间中，一项策略包括5个子策略，每个子策略由两个顺序应用的图像操作组成。另外，每个操作还关联有两个超参数：1）应用操作的概率和2）操作的幅度。这样的方法结构赋予了模型以自动发现能产生最佳验证精度的数据增强操作和其参数设置的能力

* 这段文本描述了图2中搜索空间里的一个策略示例，该策略包含5个子策略。第一个子策略指定了按顺序应用ShearX和Invert操作。应用ShearX的概率为0.9，并且一旦应用，它的幅度为7/10。然后以0.8的概率应用Invert操作。Invert操作不使用幅度信息。需要强调的是，这些操作是按照指定的顺序应用的。这一描述突出了该方法如何在遍历搜索空间时结合不同的图像处理操作，并赋予它们特定的应用概率和幅度，以形成能够有效提升网络性能的数据增强策略。

* 这段内容描述了实验中使用的图像操作和增强技术，这些操作源自流行的Python图像库PIL。为了通用性，作者考虑了PIL中所有接受图像作为输入并输出图像的函数。他们还使用了另外两种有前景的增强技术：Cutout和SamplePairing。他们搜索的操作包括ShearX/Y（剪切），TranslateX/Y（平移），Rotate（旋转），AutoContrast（自动对比度），Invert（反转），Equalize（均衡化），Solarize（曝光），Posterize（色调分离），Contrast（对比度），Color（颜色），Brightness（亮度），Sharpness（锐度），Cutout和Sample Pairing。总共有16种操作在搜索空间中。
* 每种操作都有一个默认的幅度范围，这将在第4节更详细地描述。为了能够使用离散搜索算法找到这些幅度，作者将幅度的范围离散化为10个值（均匀间距）。类似地，应用某个操作的概率也被离散化为11个值（均匀间距）。因此，寻找每个子策略成为一个在(16×10×11)^2种可能性的搜索问题。
* 作者的目标是同时找到5个这样的子策略以增加多样性。带有5个子策略的搜索空间大约有(16×10×11)^10 ≈ 2.9×10^32种可能性。这表明了一个非常广泛的搜索空间，需要一个有效的搜索算法来探索这一空间并找到最佳的数据增强组合。

* 这段内容详细描述了实验中使用的搜索算法。该算法使用了强化学习，其灵感来源于文献[71, 4, 72, 5]。搜索算法由两部分组成：控制器（一个递归神经网络）和训练算法（近端策略优化算法，Proximal Policy Optimization algorithm [53]）。
* 在每一步中，控制器通过一个softmax预测一个决策；然后将该预测作为嵌入输入到下一个步骤中。为了预测5个子策略，控制器总共需要30个softmax预测，每个子策略有2个操作，并且每个操作需要指定操作类型、幅度和概率。
* 控制器RNN的训练：控制器通过奖励信号来训练，此信号反应了策略在提高一个“子模型”（作为搜索过程的一部分被训练的神经网络）泛化能力方面的效果如何。在实验中，研究者设置了一个验证集来衡量子模型的泛化能力。子模型通过应用在训练集上的5个子策略来生成增强数据被训练（不包含验证集）。对于小批量中的每个例子，随机选择5个子策略中的一个来增强图像。然后在验证集上评估子模型以测量准确性，该准确性用作奖励信号来训练递归网络控制器。在每个数据集上，控制器大约采样15,000个策略。

* 控制器RNN的架构和训练超参数：我们按照文献[72]中的训练程序和超参数来训练控制器。更具体来说，控制器RNN是一个单层的长短期记忆网络（LSTM [21]），在每一层有100个隐藏单元，并且对于两个卷积单元（通常B为5）的每个架构决策有2×5B个softmax预测。控制器RNN的这10B个预测中的每一个都与一个概率相关联。子网络的联合概率是这些10B个softmax处所有概率的乘积。这个联合概率用来计算控制器RNN的梯度。梯度通过子网络的验证集准确性来调整，以便使控制器为表现不佳的子网络分配低概率，为表现优异的子网络分配高概率。
* 与文献[72]类似，我们使用近端策略优化（PPO）[53]作为训练算法，学习率为0.00035。为了鼓励探索，我们还使用了一个权重为0.00001的熵惩罚。在我们的实现中，基准函数是前面奖励的指数移动平均，权重为0.95。控制器的权重在-0.1和0.1之间均匀初始化。我们选择使用PPO来训练控制器是出于方便，尽管先前的研究已经表明其他方法（例如增强随机搜索和进化策略）也能表现得同样好甚至更好[30]。
* 在搜索结束时，我们将最佳5个策略中的子策略连接成一个单一策略（包含25个子策略）。这个具有25个子策略的最终策略用于训练每个数据集的模型。
上述搜索算法只是我们可以用来找到最佳策略的许多可能搜索算法之一。使用其他不同的离散搜索算法，比如基因编程[48]或者甚至随机搜索[6]，也可能改善本文的结果。

