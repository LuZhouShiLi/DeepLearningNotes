# TimesURL: Self-supervised Contrastive Learning for Universal Time Series
Representation Learning


## 摘要

&emsp;学习适用于多种下游任务的通用时间序列表示，并指出这在实际应用中具有挑战性但也是有价值的。最近，研究人员尝试借鉴自监督对比学习（SSCL）在计算机视觉（CV）和自然语言处理（NLP）中的成功经验，以解决时间序列表示的问题。然而，由于时间序列具有特殊的时间特性，仅仅依赖于来自其他领域的经验指导可能对时间序列是无效的，并且难以适应多个下游任务。


&emsp;在1和2中，研究发现不适当的正负样本构造可能引入不恰当的归纳偏差，既不能保持时间属性，也不能提供足够的区分特征。在3中，仅仅探索片段或实例级别的语义信息对于学习通用表示是不够的。


&emsp;为了解决上述问题，研究者提出了一种名为"TimesURL"的新型自监督框架。具体而言，他们首先引入了一种基于频率和时间的增强方法，以保持时间属性不变。然后，他们构建了双Universums作为一种特殊类型的硬负样本，以更好地引导对比学习。此外，他们引入了时间重建作为与对比学习共同优化的目标，以捕获片段级和实例级信息。因此，TimesURL能够学习高质量的通用表示，并在包括短期和长期预测、插补、分类、异常检测和迁移学习在内的6个不同的下游任务中取得了最先进的性能。



## Introduction

&emsp;时间序列数据在现实中无处不在，涵盖了从天气和经济到交通等多个领域。学习信息丰富且通用的时间序列表示，以应用于多类型的下游任务，是一个基本但尚未解决的问题。尽管自监督对比学习在计算机视觉（CV）、自然语言处理（NLP）以及最近在其他类型的模态中取得了巨大成功，但在时间序列中的应用需要量身定制的解决方案。这是因为时间序列数据具有高维度和特殊的时间特性，同时对于不同任务需要多样化的语义信息。


* 自监督时序数据对比学习主要是以下四个部分
  * 正样本的数据增强方法
  * 主干编码器 backbone encoder
  * 负样本对
  * 预训练任务的损失函数SSCL


&emsp;多数增强方法在应用到时间序列数据时可能引入不适当的归纳偏差，因为它们直接借鉴了计算机视觉（CV）和自然语言处理（NLP）领域的思想。例如，**一些方法（比如Flipping）可能破坏原始时间序列中固有的时间变化，如趋势和峰谷。而另一些方法（比如permutation）可能扰乱时间序列中的时间依赖关系**，从而影响过去和未来时间戳信息之间的关系。由于时间序列的有价值的语义信息主要存在于时间变化和依赖关系中，这些增强方法无法捕获对于有效的通用表示学习来说必要的特征

&emsp;由于时间序列段的局部平滑性和马尔可夫性质，大多数时间序列段可以被视为容易的负样本。这些段倾向于与锚点表现出语义上的不相似，只提供微小的梯度，因此无法提供有用的区分信息。尽管包含一小部分硬负样本（与锚点具有相似但不完全相同语义的样本）已经显示出促进改善和加速学习的效果，但由于容易获得的负样本数量过多，它们的有效性被淹没了。

&emsp;这一段讨论了硬负样本在时间序列领域的重要性。由于时间序列的局部平滑性和马尔可夫性质，大多数时间序列段对于模型来说都是相对容易的负样本。这些容易的负样本无法提供足够的梯度，从而无法为模型提供有用的区分信息。相比之下，硬负样本（与锚点具有相似但不完全相同语义的样本）的引入在其他领域已经显示出有效促进学习的作用，但在时间序列领域尚未得到充分探索。这一问题的解决对于提高自监督对比学习在时间序列数据上的性能可能是关键的。


&emsp;作者强调了仅仅使用片段级别或实例级别的信息是不足以学习通用表示的。先前的研究通常将上述任务分为两类。第一类包括预测、异常检测和插补等任务，这些任务更依赖于在段级别捕获的细粒度信息，因为它们需要推断特定的时间戳或子序列。而第二类包括分类和聚类等任务，这些任务更注重实例级别的信息，即粗粒度的信息，以便在整个序列中推断目标。

&emsp;因此，在面对一个任务无关的预训练模型，在预训练阶段缺乏特定任务的先验知识或意识的情况下，片段级别和实例级别的信息都变得不可或缺，以实现有效的通用时间序列表示学习。这一观点强调了通用表示学习需要同时考虑到时间序列数据的细粒度和粗粒度信息，以应对各种不同类型的任务。



&emsp;在论文中，为了解决这些挑战，作者提出了一种新的自监督框架，称为TimesURL，旨在学习能够有效支持各种下游任务的通用表示。为了融合时间变化和样本多样性，作者首先进行了实例级和时间对比学习。具体而言，为了保持时间的变化和依赖关系，作者设计了一种新的基于频率和时间的增强方法，称为FTAug，它是在时间域进行裁剪和在频率域进行混合的组合。此外，受到通过矛盾学习的概念启发，作者精心设计了双Universums作为硬负样本。这是一种在嵌入空间中进行的锚点特定的混合，每次将特定的正样本（锚点）与一个负样本混合。作者设计的双Universums分别在实例级和时间维度上生成，作为特殊的高质量硬负样本，提升了对比学习的性能。

&emsp;此外，作者观察到仅仅使用对比学习限制了捕获信息的层次。因此，在论文中，他们联合优化了对比学习和时间重建，以捕获和利用片段级和实例级的信息。这一方法旨在使模型能够更全面地学习时间序列的表示，从而在各种下游任务中表现更出色。

* 重新审视对比学习框架： 本文重新审视了现有的时间序列表示的对比学习框架，并提出了TimesURL，这是一个新颖的框架，通过额外的时间重建模块可以捕获片段级和实例级信息，实现通用表示学习。

* 引入新的增强方法和双Universums： 作者引入了一种新的基于频率和时间的增强方法（FTAug），并将双Universums巧妙地融入对比学习，以解决正样本和负样本构建的问题。

* 通过六项基准时间序列任务评估性能： 通过对大约15个基线的性能评估，作者评估了TimesURL学到的表示在六个基准时间序列任务上的性能。持续的最先进性能证明了该表示的通用性。



## Related Work

* SPIRAL（Lei et al. 2019）： SPIRAL通过学习一个特征表示，有效地保留了原始时间序列数据中固有的成对相似性，从而弥合了时间序列数据与静态聚类算法之间的差距。

* TimeNet（Malhotra et al. 2017）： TimeNet是一个循环神经网络，通过训练编码器-解码器对来最小化从其学到的表示到原始时间序列的重构误差。

* DTCR（Ma et al. 2019）： DTCR将时间重构和K-means目标集成到seq2seq模型中，以学习特定于聚类的时间表示。

* ROCKET（Dempster, Petitjean, and Webb 2020）： ROCKET是一种分类方法，具有较小的计算成本和快速的速度，它使用随机卷积核转换时间序列，并使用转换后的特征训练线性分类器。


* 这些先前的研究主要集中于开发编码器-解码器架构，以最小化无监督时间序列表示学习中的重构误差。有些研究尝试利用时间序列数据中存在的内在相关性，但并未充分发挥时间序列数据的潜力。这表明在无监督时间序列表示学习方面仍然存在挑战和改进的空间

```java
TS-TCC（Eldele et al. 2021）： 专注于为时间序列数据设计具有挑战性的预训练任务，通过设计一个跨时间戳和增强引入的艰难的跨视图预测任务来实现鲁棒的表示学习。

TNC（Tonekaboni, Eytan, and Goldenberg 2021）： 讨论了对非平稳多变量时间序列进行正负样本构造的选择，通过一种新颖的基于邻域的方法，并进行样本权重调整。

InfoTS（Luo et al. 2023）： 强调了选择适当的增强方法的重要性，并设计了一种自动选择增强方法的元学习方法，以防止引入预先制定的知识。

TS2Vec（Yue et al. 2022）： 是一个统一的框架，学习不同语义层次上的任意子序列的上下文表示。

CoST（Woo et al. 2022）： 通过利用模型架构中的归纳偏差，为预训练任务设计做出贡献。具体而言，CoST专注于学习解耦的季节性和趋势表示，并引入了一种新颖的频域对比损失以促进具有区分性的季节性表示。

```

## Proposed TimesURL Framework


### 问题陈述


&emsp;问题陈述。与大多数时间序列表示学习方法类似，作者的目标是学习一个非线性嵌入函数 fθ，使得时间序列集合 X = {x1, x2, . . . , xN } 中的每个实例 xi 能够映射到最佳描述的表示 ri。每个输入时间序列实例 xi ∈ R^T×F，其中 T 是时间序列的长度，F 是特征的维度。第 i 个时间序列的表示是 ri = {ri,1, ri,2, . . . , ri,T }，其中 ri,t ∈ R^K 是在时间 t 的表示向量，其中 K 是表示向量的维度。由于模型是一个两步的过程，接下来使用学到的表示来完成下游任务。


* 方法介绍。如图1所示，首先通过 FTAug 分别为原始序列 X 和掩码序列 XM 生成增强集合 X' 和 X'M。然后，我们得到两对原始和增强序列集合，第一对（X，X'）用于对比学习，而第二对（XM，X'M）用于时间重建。之后，我们使用 fθ 将上述集合映射到相应的表示。我们鼓励 R 和 R' 具有变换一致性，并设计了一种重建方法，通过 RM 和 R'M 精确地恢复原始数据集 X。上述模型的有效性由以下因素保证：1）对于正样本构建使用合适的增强方法，2）具有一定数量的硬负样本以实现模型的泛化，以及3）通过对比学习和时间重建损失联合优化编码器 fθ，以捕获两个层次的信息。接下来，我们将在以下子部分中讨论这三个部分。

### FTAug

* 对比学习的一个关键组成部分是选择适当的增强方法，以施加一些先验条件来构建可行的正样本，从而使编码器能够学习到鲁棒且有区分性的表示（Chen et al. 2020; Grill et al. 2020; Yue et al. 2022）。大多数增强策略都是依赖于任务的（Luo et al. 2023），并且可能引入对数据分布的强烈假设。更严重的是，它们可能扰动对于像预测这样的任务至关重要的时间关系和语义一致性。因此，作者选择了上下文一致性策略（Yue et al. 2022），它将两个增强上下文中相同时间戳的表示视为正样本。作者的 FTAug 结合了频率和时间领域的优势，通过频率混合和随机裁剪生成增强的上下文。


### 频率混合

&emsp;频率混合。频率混合用于通过用同一批次中另一个随机训练实例 xk 的相同频率分量替换通过快速傅里叶变换（FFT）操作计算的某一训练实例 xi 的一定比例的频率分量，从而生成一个新的上下文视图。然后，使用逆FFT将其转换回以获取新的时域时间序列（Chen et al. 2023）。在样本之间交换频率分量不会引入意外的噪音或人工周期性，并且可以提供更可靠的增强，以保留数据的语义特征。

### 随机裁剪

&emsp;随机裁剪。随机裁剪是上下文一致性策略的关键步骤。对于每个实例 xi，我们随机采样两个重叠的时间段 [a1, b1]、[a2, b2]，使得 0 < a1 ≤ a2 ≤ b1 ≤ b2 ≤ T。对比学习和时间重建进一步优化了重叠段 [a2, b1] 中的表示。最终，所提出的 FTAug 对于各种任务都是有益的，因为它可以保持时间序列的重要时间关系和语义一致性。需要注意的是，FTAug 仅在训练过程中应用。

### Double Universum Learning

&emsp;近期研究（Kalantidis et al. 2020; Robinson et al. 2020; Cai et al. 2020）揭示了在对比学习中，硬负样本发挥着重要作用，但在时间序列领域尚未得到充分探讨。此外，由于时间序列中的局部平滑性和马尔可夫性质，大多数负样本都是容易负样本，这些样本不足以捕捉时序信息，因为它们基本上缺乏驱动对比学习所需的学习信号。在图2中以UEA档案中的ERing数据集为例（Bagnall et al. 2018），对于每个正锚点（红色方块），相应的负样本（灰色标记）包含许多容易负样本和少量的困难负样本，即许多负样本距离太远，无法为对比损失做出贡献。

&emsp;我们的双Universums是在实例级和时序级都进行混合的混合诱导的Universums（Han and Chen 2023; Vapnik 2006; Chapelle et al. 2007）。这是嵌入空间中的特定于锚点的混合，将特定的正样本特征（锚点）与未标记数据的负样本特征混合在一起。

* 如图2(a)所示，大多数Universum（蓝色三角形）与锚点距离较近，因此可以视为难负样本。此外，我们利用了一个代理任务来指示难负样本（Kalantidis et al. 2020），即Universums的难度。代理任务的性能如图2(b)所示，即在ERing数据集上使用和不使用Universums训练我们的TimesURL时，正样本在所有负样本中的排名百分比。尽管TimesURL的代理任务性能有所下降，但在线性分类中观察到了进一步的性能提升，从0.896（没有Universums）提高到0.985（有Universums），这意味着额外的Universum使代理任务更难解决，但可以进一步提高模型在下游任务中的性能。因此，TimesURL中的Universums可以被视为高质量的负样本。总的来说，我们的Universums可以被看作是一种高质量的难负样本。




&emsp;通过与锚点样本混合，Universum 数据落入数据空间中的目标区域的可能性被最小化，从而确保了 Universum 的难负性。此外，双 Universum 集合包含了所有其他负样本，有助于学习有区分性的样本信息以增加模型的能力。


### Contrastive Learning for Segment-level Information


&emsp;这两个损失相辅相成，以捕捉实例特定的特征和时序变化。我们使用分层对比损失（Yue et al. 2022），通过在方程（3）和（4）中沿时间轴对学到的表示进行最大池化，从而进行多尺度信息学习。在这里，我们必须提到的是，重要的时序变化信息，如趋势和季节性，在经过几次最大池化操作后会丢失，因此在顶层进行对比实际上不能捕获足够的实例级信息供下游任务使用。

### Time Reconstruction for Instance-level Information


&emsp;
在自监督学习中，掩码自编码技术在各个领域都被证明在性能上表现良好，例如在NLP中基于BERT的预训练模型（Kenton and Toutanova 2019）以及在CV中的MAE（He et al. 2022）。这类方法的主要思想是在给定部分观察的情况下重建原始信号。

受到掩码自编码技术的启发，我们设计了一个重建模块来保留重要的时序变化信息。我们的方法使用上述提到的嵌入函数 fθ 作为编码器，将被掩码的实例映射到潜在表示，然后从潜在表示重构完整实例。在这里，我们采用随机掩码策略。我们的损失函数计算重建值与每个时间戳上的原始值之间的均方误差（MSE）。此外，类似于BERT和MAE，我们在方程（6）中仅在被掩码的时间戳上计算MSE损失。


## 分类


&emsp;实验设置：
时间序列分类在医学诊断、动作识别等领域具有实际意义。实验在类标签分配给每个实例的情况下进行。采用实例级分类来评估模型在表示学习中的能力。选择了常用的UEA（Bagnall等人，2018年）和UCR（Dau等人，2019年）分类档案中的数据集。所有分类方法的表示维度，除了DTW，都设置为320。协议遵循TS2Vec，该协议使用在表示上训练的具有RBF核的SVM分类器进行分类。

结果：
表1中的评估结果显示TimesURL取得了最佳性能。在UEA中，对30个单变量数据集的平均准确率达到了75.2％，在UCR中，对128个多变量数据集的平均准确率为84.5％，超过了先前的自监督方法InfoTS（71.4％）。此外，最佳平均排名进一步验证了TimesURL的显著优越性。

如前所述，其他方法的失败很容易理解：TS2Vec缺乏足够的实例级信息，而InfoTS中的一些数据增强可能引入不适当的归纳偏差，损害了对于分类而言至关重要的时序属性，如趋势。此外，其他对比学习方法，包括T-Loss、TS-TCC、TNC和TST，仅在某个水平上表现。由于TimesURL使用通用的FTAug，包含适当的难负样本，并能够捕捉段和实例级信息，因此实现了更好的性能。


## 总结
&emsp;本文提出了一种名为TimesURL的新型自监督框架，可以学习适用于各种下游任务的通用时间序列表示。我们引入了一种名为FTAug的新型增强方法，以保持上下文一致性和时序特性不变，适用于各种下游任务。此外，我们引入了双Universums用于对比学习，以提高负样本的数量和质量，从而增强对比学习的性能。最后，TimesURL联合优化对比学习和时间重构，以捕捉通用表示学习的段级和实例级信息。实验证明了上述策略的有效性，并表明通过合适的增强方法、足够的难负样本和适当的信息级别，TimesURL在六个下游任务上表现出色。


























