# Time Series Contrastive Learning with Information-Aware Augmentations


## 摘要

* 背景：在近年来，已经有许多对比学习方法被提出，并在实证上取得了显著的成功。
尽管对比学习在图像和语言领域非常有效和普遍，但在时间序列数据上的应用相对较少。
对比学习的关键组成部分：

* 对比学习的一个关键组成部分是选择适当的数据增强（augmentation）方式，通过施加一些先验条件构建可行的正样本。这样，编码器可以通过训练来学习稳健和具有区分性的表示。
问题陈述：

* 与图像和语言领域不同，时间序列数据的“期望”增强样本很难通过人为的先验条件来生成，因为时间序列数据具有多样且人类难以识别的时间结构。如何为给定的对比学习任务和数据集找到对时间序列数据有意义的增强仍然是一个未解的问题。
* 解决方法：作者通过信息理论提出了一种方法，旨在同时鼓励高保真度和多样性，从而产生有意义的时间序列数据增强。通过理论分析，提出了选择可行数据增强的标准。

* 新方法提出：作者提出了一种新的对比学习方法，名为InfoTS，该方法使用信息感知的增强（information-aware augmentations），能够自适应地选择最优的增强方式用于时间序列表示学习。
* 实验证明：在各种数据集上的实验表明，该方法在预测任务上能够实现高竞争性的性能，减少了均方误差（MSE）高达12.0%。在分类任务上相对于主流基线方法，准确度提高了最高达3.7%。



## Introduction


* 时间序列数据的特性：时间序列数据在现实世界中通常是高维的、非结构化的，具有独特的属性。
其复杂性和缺乏结构性使得在数据建模方面面临挑战（引用了 Yang 和 Wu 在 2006 年的工作）。
* 标注难题：相比于图像和语言数据，时间序列数据通常没有人类能够轻松识别的模式。
在实际应用中，由于这些标注难以实现，很难为时间序列数据添加准确的标签。
这种标注的困难性限制了深度学习方法的应用，因为这些方法通常需要大量标记数据进行训练，而在时间序列数据上很难获得足够的标记数据（引用了 Eldele 等人在 2021 年的工作）。
* 表示学习的重要性：为了克服标注的限制，提到了表示学习的概念。表示学习通过从原始时间序列数据中学习到的固定维度嵌入，保留其固有特征。
相对于原始时间序列数据，这些表示具有更好的可传递性和泛化能力。
* 对比学习方法的应用：为了应对标注的限制，该文提到在各个领域广泛采用了对比学习方法。
对比学习在视觉、语言和图结构数据等领域表现出色，因为它在表示学习方面具有出色的性能。
对比学习方法通常通过训练一个编码器（encoder）将实例映射到一个嵌入空间，其中不相似（负面）实例容易与相似（正面）实例区分开来。


* 对比学习在时间序列领域的不足：尽管对比学习在其他领域取得了成功，但在时间序列领域却受到相对较少的探索。引用了一些相关的研究工作（Eldele等人在2021年，Franceschi等人在2019年，Fan等人在2020年，Tonekaboni等人在2021年）。
* 现有对比学习方法的局限性：现有的对比学习方法通常包括特定的数据增强策略，这些策略通过创建新的、看起来真实的训练数据，但不改变其标签，为任何输入样本构建正样本的替代。
这些方法的成功依赖于由领域专业知识指导的精心设计的经验法则对比学习中通常使用的数据增强主要是为图像和语言数据设计的，如颜色扭曲、翻转、词替换和回译等，这些技术通常不适用于时间序列数据。
* 时间序列数据的特殊性：与图像不同，时间序列数据通常关联着难以解释的潜在模式。
强大的数据增强方法（如置换）可能破坏这些模式，导致模型将负手工制作的样本误认为正样本。
弱的数据增强方法（如抖动）可能生成的增强实例与原始输入过于相似，难以为对比学习提供足够的信息。
* 现有方法的局限性和挑战：现有方法存在两个主要限制。首先，与具有人类可识别特征的图像不同，时间序列数据通常具有难以解释的潜在模式。
其次，来自不同领域的时间序列数据可能具有多样的性质，将一个通用的数据增强方法应用于所有数据集和任务可能导致次优性能。
一些方法采用经验法则从昂贵的试错中选择适当的增强，但这种手动选择不如从学习的角度来说是不可取的。


* 数据增强的目的：数据增强有助于实现可推广、可传递和鲁棒的表示学习，通过将输入训练空间正确外推到一个更大的区域（引用了 Wilk 等人在 2018 年的工作）。
正实例围绕一个具有判别性的区域，其中所有数据点应与原始实例相似。


* 期望的数据增强特性：对于对比表示学习，期望的数据增强应同时具有高保真度和高多样性。
高保真度鼓励增强数据保持语义标识的不变性，例如，在分类任务中，生成的增强输入应该保持类别不变。高多样性有助于通过增加泛化能力来进行表示学习



* 理论分析基础：作者通过信息论在数据增强中进行了信息流的理论分析，并导出了选择理想的时间序列增强的标准。由于实际时间序列数据的不可解释性，作者假设语义标识由下游任务中的目标表示。因此，高保真度可以通过最大化下游标签与增强数据之间的互信息来实现。

* 高保真度的实现：在无监督设置下，当下游标签不可用时，给每个实例分配一个独热伪标签。
这些伪标签鼓励不同实例的增强之间可区分性


* 高多样性的实现：同时，作者通过在原始实例条件下最大化增强数据的熵，以增加数据增强的多样性。

* 自适应数据增强方法 - InfoTS：基于导出的标准，作者提出了一种自适应数据增强方法，即InfoTS。图1展示了该方法的架构。InfoTS的设计旨在避免临时选择或繁琐的试错调整。

* 元学习器（Meta-Learner）的使用：作者在InfoTS中引入了另一个神经网络，称为元学习器，用于与对比学习一同学习数据增强的先验知识。元学习器自动选择从候选增强中生成可行正样本的最佳增强。增强后的实例与随机采样的负实例一同输入时间序列编码器，以对比学习的方式学习表示。

* Reparameterization Trick：通过重新参数化技巧，元学习器可以根据提出的标准进行高效优化，并通过反向传播来学习。这使得元学习器能够在不倚赖专业知识或繁琐的下游验证的情况下，以每个数据集和每个学习任务的方式自动选择数据增强

## Methodology


## Information-Aware Criteria for Good Augmentations


* 数据增强的目标：对比学习的数据增强目标是创建具有现实合理性的实例，通过不同的转换方法保持语义。与视觉和语言领域的实例不同，时间序列数据的底层语义对人类来说并不可识别，因此很难，甚至是不可能，将人类知识纳入时间序列数据的数据增强过程中。

* 难以人为引导的增强：举例说明，旋转图像不会改变其内容或标签，而对一个时间序列实例进行排列可能破坏其信号模式并生成一个无意义的时间序列实例。现实生活中时间序列数据集的巨大异质性使得基于试错选择的方法变得不切实际。



```java
数据增强的目标：

对比学习的数据增强目标是创建具有现实合理性的实例，通过不同的转换方法保持语义。
与视觉和语言领域的实例不同，时间序列数据的底层语义对人类来说并不可识别，因此很难，甚至是不可能，将人类知识纳入时间序列数据的数据增强过程中。
难以人为引导的增强：

举例说明，旋转图像不会改变其内容或标签，而对一个时间序列实例进行排列可能破坏其信号模式并生成一个无意义的时间序列实例。
现实生活中时间序列数据集的巨大异质性使得基于试错选择的方法变得不切实际。
对比表示学习中理想的数据增强：

从作者的角度来看，对比表示学习中的理想数据增强应该保持高保真度、高多样性，并且能够适应不同的数据集。
图示和示例：

通过图2中的插图和示例说明了上述理念。

```

## 高保真


* 高保真度的概念：高保真度的增强应该能够维持在转换中不变的语义标识。
由于在实际时间序列数据中，语义的可识别性较差，因此通过目视检查增强的保真度是具有挑战性的。
* 假设和定义：作者假设时间序列实例的语义标识可以由其在下游任务中的标签表示，该标签在训练期间可能是可用或不可用的。
在受监督情况下，作者从保持高保真度的目标开始分析，并稍后将其扩展到无监督情况。
* 目标定义：受到信息瓶颈理论的启发，作者定义了一个目标，即保持高保真度，其形式是增强 v 与标签 y 之间的互信息（Mutual Information，MI）较大，即 MI(v; y)。



### 增强函数 v 的定义：

作者将增强函数 v 视为输入 x 和随机变量  的概率函数，表示为 v = g(x;  )。
### 互信息的定义：

从互信息的定义出发，有 MI(v; y) = H(y) - H(y|v)，其中 H(y) 是标签 y 的（Shannon）熵，H(y|v) 是在增强 v 条件下的标签 y 的熵。


### 目标函数：

由于 H(y) 与数据增强无关，因此目标等价于最小化条件熵 H(y|v)。
为了更高效地优化，作者采用了类似于 (Ying et al. 2019) 和 (Luo et al. 2020) 的方法，将条件熵近似为标签 y 和预测标签 ˆy 之间的交叉熵。


### 交叉熵的使用：

为了有效地近似条件熵，作者选择使用交叉熵作为衡量标签预测与真实标签之间的差异的度量。这里，ˆy 是通过将增强 v 作为输入计算得到的预测标签。





## 增强之后的保真性

性质 1 - 保持保真度：

如果增强函数 v 保持独热编码伪标签，那么增强 v 与下游任务标签 y 之间的互信息（尽管在训练中不可见）等同于原始输入 x 与标签 y 之间的互信息，即 MI(v; y) = MI(x; y)。
独热编码伪标签的关键：

这个性质的关键在于保持了在无监督学习中引入的独热编码伪标签的信息。独热编码伪标签的引入是为了在对比学习中构建正样本。
互信息的等价性：

在这个性质下，作者指出，如果增强 v 保持了与原始输入 x 相关的独热编码伪标签，那么在对比学习任务中，v 与 y 之间的互信息与 x 与 y 之间的互信息是等价的。

性质 2 - 添加新信息：

如果保持独热编码伪标签，增强 v 相对于原始输入 x 包含了新的信息，即熵 H(v) ≥ H(x)。
这表示，通过在对比学习中引入独热编码伪标签，增强后的实例相对于原始实例具有更多的信息。
证明和详细信息：

文中提到了有关这一性质的详细证明在附录中，可以进一步查阅附录以获取更详细的解释。
性质的意义：

这些性质说明在无监督学习中，通过保持独热编码伪标签，确保生成的增强实例在对比学习中不会降低保真度，而且可能引入新的信息，有助于对比学习的效果。
优化效率问题：

在无监督学习中，数据集 X 中的标签数等于实例数，直接优化公式（2）是低效且不可扩展的。为了提高效率，作者使用批次的独热编码 yB 近似 y，将标签数 C 从数据集大小减少到批次大小。


## 高多样性


* 高多样性的重要性：文中指出，增强中存在足够的变化可以提升对比学习模型的泛化能力。在对比学习中，泛化能力指模型在未见过的数据上的性能。
* 信息论中的不确定性：信息论中，随机变量可能结果的不确定性由其熵来描述。熵越高，表示不确定性越大。
* 条件熵的概念：文中提到，为了保持增强的高多样性，作者通过最大化在给定原始输入 x 的条件下增强 v 的条件熵 H(v|x)。
* 条件熵的定义：条件熵是在给定一些其他信息的情况下，关于某个事件的不确定性。在这里，文中没有给出具体的条件熵的定义，但这通常表示为 H(v|x)。

## InfoMin

InfoMin的设计原理：

InfoMin的设计基于信息瓶颈理论，即良好的视图应该从原始输入中保留最小而足够的信息。InfoMin假设增强的视图是输入的函数，这在很大程度上限制了数据增强的变化。
与InfoMin的类比：

与信息瓶颈类似，InfoMin假设增强的视图是输入的函数，这严重限制了数据增强的变化。此外，在无监督设置中，高保真度属性被忽略。
对时间序列数据的适用性：

文中指出，InfoMin在图像数据集中效果良好，因为那里有人类知识的可用性。然而，在时间序列数据中，它可能无法生成合理的增强。
对信息增强多样性的处理方法：

InfoMin采用对抗学习来最小化互信息的下界，以增加增强的多样性。相比之下，作者的方法倾向于最小化统计依赖性，更倾向于使用上界（如L1Out），而不是下界。



## Time Series Meta-Contrastive Learning


### InfoMin的设计原理：

InfoMin的设计基于信息瓶颈理论，即良好的视图应该从原始输入中保留最小而足够的信息。InfoMin假设增强的视图是输入的函数，这在很大程度上限制了数据增强的变化。

### 与InfoMin的类比：

与信息瓶颈类似，InfoMin假设增强的视图是输入的函数，这严重限制了数据增强的变化。此外，在无监督设置中，高保真度属性被忽略。
对时间序列数据的适用性：

文中指出，InfoMin在图像数据集中效果良好，因为那里有人类知识的可用性。然而，在时间序列数据中，它可能无法生成合理的增强。
对信息增强多样性的处理方法：

InfoMin采用对抗学习来最小化互信息的下界，以增加增强的多样性。相比之下，作者的方法倾向于最小化统计依赖性，更倾向于使用上界（如L1Out），而不是下界。


### 结构

* 编码器架构：

采用的编码器表示为 fθ(x)：R^T × F → R^D，其中 x 是时间序列实例，T 是序列长度，F 是特征的维度，D 是表示向量的维度。
组件：

* 编码器包含两个主要组件：
全连接层
10层的扩张卷积神经网络模块
* 时间序列结构的探索：

为了探索时间序列的内在结构，采用了全连接层和扩张卷积神经网络模块。
对比学习框架中的损失：
**在对比学习框架中，为了训练编码器，采用了全局层面（实例级别）和局部层面（子序列级别）的损失。**


### 全局损失

全局层面的对比损失设计：

全局层面的对比损失旨在捕捉时间序列数据集中实例级别的关系。
正负对的定义：

对于给定的时间序列实例批次（Batch）XB ⊆ X，对于每个实例 x ∈ XB，生成一个通过自适应选择的变换得到的增强实例 v。这里 (x, v) 被视为正对（positive pair），而其他 (B−1) 组合 {(x, v0 )}，其中 v0 是 x0 的增强实例，且 x0 = x，被视为负对（negative pairs）。
基于InfoNCE的对比损失：

采用了基于InfoNCE（Noise-Contrastive Estimation）的全局层面对比损失设计，其灵感来自于先前的工作（Hjelm et al.，2018）。
批次实例级别的对比损失：

文中没有提供具体的对比损失公式，但提到了根据InfoNCE设计的批次实例级别的对比损失。


### 局部损失

局部层面对比损失的设计：

局部层面的对比损失旨在探索时间序列中的时序内部关系。
增强实例的处理：

对于时间序列实例 x 的增强实例 v，首先将其分割成一组子序列 S，每个子序列的长度为 L。
正对的生成：

对于子序列 s ∈ S，采用类似于 (Tonekaboni, Eytan, and Goldenberg 2021) 的方法生成一个正对（positive pair）(s, p)，其中选择了与 s 靠近的另一个子序列 p。
负对的生成：

采用了非相邻样本（¯Ns）来生成负对（negative pairs）。
局部层面对比损失公式：

文中未提供具体的局部层面对比损失公式，但表明了如何通过正对和负对的生成来定义该损失。详细的描述可以在附录中找到

### Meta-learner Network


元学习网络的背景：

先前的时间序列对比学习方法通常采用规则或繁琐的试错方法生成数据增强，这些方法要么依赖于预先制定的人类先验知识，要么需要大量的试验和错误。作者在这部分讨论了如何通过元学习网络来基于提出的信息感知准则自适应地选择最优的增强方式。
元学习网络的作用：

元学习网络被用来动态选择最优的数据增强，从而避免了对特定数据集和学习任务进行设计的繁琐过程。作者将元学习网络的选择看作一种先验选择，以适应性地确定最优的增强方式。
候选变换和权重：

选择了一组候选变换 T，例如 jittering 和 time warping。每个候选变换 ti ∈ T 都与一个权重 pi ∈ (0, 1) 相关联，表示选择变换 ti 的概率。
增强实例的计算：

对于给定的实例 x，通过变换 ti 生成的增强实例 vi 的计算方式未在这里给出，但表示了作者如何使用元学习网络和候选变换集来计算最终的增强实例。


## Related Work


### 对比学习在表示学习中的应用：

对比学习在各个领域中都得到了广泛的应用，取得了卓越的性能，包括图像、语言等领域（Chen et al. 2020; Xie et al. 2019; You et al. 2020）。
### 时间序列领域的对比学习研究：

近年来，一些研究工作开始将对比学习应用到时间序列领域（Oord, Li, and Vinyals 2018; Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Eldele et al. 2021; Tonekaboni, Eytan, and Goldenberg 2021; Yue et al. 2021）。
### 时间对比学习的方法：

时间对比学习使用多项式逻辑回归分类器训练特征提取器，以区分时间序列中的所有段落（Hyvarinen and Morioka 2016）。
一些方法生成正对和负对，基于子序列进行对比学习（Franceschi, Dieuleveut, and Jaggi 2019; Tonekaboni, Eytan, and Goldenberg 2021）。
TNC采用了去偏的对比目标，以确保在表示空间中，局部邻域内的信号能够与非邻域内的信号区分开（Tonekaboni, Eytan, and Goldenberg 2021）。
SelfTime通过探索样本间和样本内关系，采用多种手工制作的增强方法进行无监督时间序列对比学习（Fan, Zhang, and Gao 2020）。
TS2Vec以分层方式学习每个时间戳的表示，并进行对比学习（Yue et al. 2021）。
### 方法的限制：

这些方法中的数据增强方法要么是通用的，要么是通过错误和尝试来选择的，因此在复杂的实际生活数据集中的广泛应用受到了限制。


## 自适应数据增强

数据增强的重要性：

数据增强是对比学习中的重要组成部分，有助于提高模型的性能。
自适应数据增强的背景：

其他领域的研究表明，最佳增强的选择取决于下游任务和数据集（Chen et al. 2020; Fan, Zhang, and Gao 2020）。
视觉领域的自适应数据增强方法：

在视觉领域，一些研究者已经探索了自适应选择最佳增强的方法。AutoAugment通过强化学习方法自动搜索平移策略的组合（Cubuk et al. 2019）。Faster-AA通过可微分策略网络改进了数据增强的搜索管道（Hataya et al. 2020）。DADA进一步引入了无偏梯度估计器，实现了高效的一遍优化策略（Li et al. 2020）。
对比学习框架中的信息瓶颈理论：

在对比学习框架中，Tian等人应用信息瓶颈理论，该理论认为最佳视图应该共享最小且足够的信息，以指导对比学习中的良好视图的选择（Tian et al. 2020）。
对于时间序列领域的不同方法：

由于时间序列数据的复杂性，直接应用信息瓶颈框架可能会在增强过程中保留不足够的信息。与之不同的是，作者聚焦于时间序列领域，并提出了一种端到端可微分的方法，以自动选择每个数据集的最佳增强方法。


## 实验

### 评估设置:

通过有监督的时间序列分类来评估表示的质量。
使用具有径向基函数内核的SVM分类器对训练集中的表示进行训练。
训练后的分类器用于在测试集上进行预测。
### 数据集和基线方法:

使用两种类型的基准数据集：包含128个单变量时间序列数据集的UCR档案和包含30个多变量数据集的UEA档案。
进行比较的基线方法包括TS2Vec、T-Loss、TS-TCC、TST和DTW。
InfoTS与这些基线进行比较，并且它在表示学习方面采用了纯粹的无监督设置。
### 性能结果:

在UEA数据集上的结果总结在表2中（完整结果在附录中）。
在由地面真相标签引导的情况下，InfoTSs显著优于其他基线。平均而言，它比最佳基线（TS2Vec）提高了3.7%的分类准确度。
在纯粹的无监督设置下，InfoTS实现了第二好的平均性能，展示了它保持保真度的能力。
在UCR数据集上的结果（单变量，具有易于识别的模式）也在附录的表8中提供。尽管数据增强在这些数据集上具有边际或负面效果，但基于我们的标准为每个数据集自适应选择的增强方法（InfoTSs和InfoTS）仍然优于现有技术。




