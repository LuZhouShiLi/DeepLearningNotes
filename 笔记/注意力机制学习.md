# 注意力机制的快速学习

## 注意力机制


&emsp;将焦点聚焦在比较重要的事物上

* 我（查询对象Q），这张图（被查询对象V）

* 我看一张图，第一眼，就会判断那些东西对我而言比较重要，那些对于我不重要（去计算Q和V之间的事物重要度）

* 重要度计算，其实就是计算相似度（更接近），点乘其实就是计算内积

* Q，K = k1,k2,k3...kn,我们一般使用点乘方式


* 通过点乘的方法计算Q和K里面的每一个事物的相似度，就可以拿到Q和k1的相似度s1,Q和k2的相似度s2,... 相似度sn


* 最后，softmax(s1,s2,...,sn)就可以得到概率(a1,a2,..,an)，然后就知道那个更加重要


![图 0](../images/d1e59494e4ac4cafde285c0c98429d2be99f1d9a75efede3feae21ee174c6635.png)  

* 原先的V = （v1,v2,vn）

* 最后进行一个汇总，当使用Q查询结束之后，Q已经失去它的使用价值，最后还是需要使用这张图片，但是现在的这张图片多了一些信息

* （a1,a2,an）* (v1,v2,vn) = (a1*v1,a2*v2...)
* 这样就得到一个新的V，新的V就包含了哪些更重要的信息，那些不重要的信息
* 使用新的V代替原来的V

## 自注意力机制

* 第一眼看一张图，不会把所有的信息都看完


![图 1](../images/ecc35a8bf711628eaad3aa05ec044fe12630de83c8128a5f7ac111538272e3a3.png)  

* QK相乘计算相似度，然后做一个scale（后面softmax的时候避免极端情况）
* softmax得到概率

* 自注意力机制的关键点在于，不仅仅是K=V=Q 来源于同一个X,三者是同一个源头

![图 2](../images/2a522c28d58f5168b28aeebfd2aecd5d96d93d1876aeb778eda1ff88f9356369.png)  


* Q KV 的获取是通过三个参数Wq,  Wk,Wv进行矩阵相乘得到的
* 接下来的步骤和注意力机制一样

![图 3](../images/5ec0d1e44fe8cadfd8d1818d8d1fcb1b6c02a530ed59502c0f4c4f4ff7bc2373.png)  

![图 4](../images/82d8c68272b74cc5ba4cfcaa7dd8771ed0891dc2a4019b6540caa60827ab2dce.png)  


* z1就是达标thinking的新的向量表示


![图 5](../images/1b6c4cd8d6dadfd758e46825d1eee9480900c7aa969e445e106fe1a7db996b12.png)  


![图 6](../images/51f78e25ceea0df72f0aa8c8b47128dc4e1621915d7ec23fda4dda15c0bb274a.png)  


