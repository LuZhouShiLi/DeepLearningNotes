# 读写文件


## 加载和保存张量

&emsp;对于单个张量 我么可以直接调用load和save函数分别读写，这两个函数要求我们提供一个名称，save要求保存的变量作为输入

```py
import torch
from torch import nn
from torch.nn import functional as F

# 创建一个长度为4的张量
x = torch.arange(4)
torch.save(x, 'x-file')


x2 = torch.load('x-file')
print(x2)

```


存储一个张量列表，然后把他们写入内存

```py
y = torch.zeros(4)
torch.save([x,y],'x-files')
x2,y2 = torch.load('x-files')
(x2,y2)

```


我们甚至可以写入或者读取从字符串映射到张量的字典，方面读取权重

```py
# 创建张量字典  保存张量
mydict = {'x':x,'y':y}
torch.save(mydict,'mydict')

mydict2 = torch.load('mydict')
mydict2

```


## 加载和保存模型参数

&emsp; 深度学习框架提供内置函数来保存和加载整个网络，这里是保存模型的参数而不是保存整个模型

```py
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20,256)
        self.output = nn.Linear(256,10)

    def forward(self,X):
        return self.output(F.relu(self.hidden(X)))
    
net = MLP()
X = torch.randn(size= (2,20))
Y = net(X)

```

取出模型的参数保存在一个mlp.params文件中

```py
# 取出模型的参数保存在一个mlp.params文件中
torch.save(net.state_dict(),'mlp.params')

```

&emsp; 恢复模型，实例化原始多层感知机模型的一个备份，我们不需要随机初始化模型参数，而是直接读取文件中存储的参数

```py
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()

```

&emsp;比较两个对象的模型参数，那么输入相同的X 计算的输出应该相同

```py
Y_clone = clone(X)
Y_clone == Y

```




