# Faster AutoAugment: Learning Augmentation Strategies using Backpropagation


## 摘要


&emsp;数据增强方法是提升深度神经网络性能的不可或缺的启发式方法，特别是在图像识别任务中。最近的几项研究表明，通过搜索算法找到的增强策略优于手工制定的策略。这些方法利用黑盒搜索算法在具有连续或离散参数的图像转换上进行搜索，并且需要很长时间才能获得更好的策略。在本文中，我们提出了一种不同iable策略搜索管道用于数据增强，该管道比先前的方法要快得多。我们介绍了几种具有离散参数的转换操作的近似梯度，以及选择操作的可微机制。作为训练的目标，我们最小化了增强数据的分布与原始数据之间的距离，这可以被微分。我们展示了我们的方法 Faster AutoAugment 实现了显著更快的搜索速度，而没有性能下降。


## Introduction


&emsp;数据增强是一种强大的技术，用于机器学习中增加数据量和多样性，特别是在图像识别任务中。传统的数据增强方法包括几何变换（如旋转）和颜色增强（如自动对比度）。与其他超参数类似，数据增强策略的设计者通常根据其先前的知识（例如所需的不变性）选择变换操作。例如，水平翻转预计对一般物体识别有效，但可能对数字识别无效。除了选择之外，设计者还需要组合几种操作并设置它们的大小（例如旋转角度）。因此，设计数据增强策略是一个复杂的组合问题

&emsp;在本文中，我们提出通过近似梯度信息来解决这个问题，从而实现基于梯度的优化数据增强策略。为此，我们使用直通估计器来近似离散图像操作的梯度，并通过结合最近的可微分神经结构搜索方法使操作选择过程可微分。作为目标，我们最小化原始图像和增强图像之间的分布距离，因为我们希望数据增强管道能够转换图像，填补训练数据中缺失的点。为了使转换后的图像与原始图像的分布匹配，我们使用了对抗学习。因此，搜索过程变得端到端可微分，并且比先前的工作（如AutoAugment、PBA和Fast AutoAugment）显著更快。


&emsp;我们在标准基准测试数据集 CIFAR-10、CIFAR-100、SVHN 和 ImageNet 上经验性地展示了我们的方法，我们称之为 Faster AutoAugment，能够在实现可比较的性能的同时实现更快的策略搜索。

总之，我们的贡献有以下三点：

* 我们为几种不可微分的数据增强操作引入了梯度近似方法。
* 我们通过梯度近似方法、可微操作的选择以及一个可微分的目标函数，使数据增强策略的搜索变得端到端可微分，该目标函数衡量原始图像和增强图像分布之间的距离。
* 我们展示了我们提出的方法 Faster AutoAugment 在不降低性能的情况下显著缩短了搜索时间，相比之前的方法。

## 2. Related Work



### Neural Architecture Search

&emsp;神经架构搜索（NAS）旨在自动设计神经网络的架构，以实现比手动设计的更高性能。为此，NAS 算法需要使用搜索算法（如强化学习和进化策略）从离散搜索空间中选择更好的组件组合（例如，具有 3x3 核的卷积）。

最近，DARTS 通过将离散搜索空间放宽到连续空间，实现了更快的搜索，这使他们能够使用基于梯度的优化。而 AutoAugment 受到 [38] 的启发，我们的方法受到 DARTS 的影响。


### Data Augmentation

&emsp;数据增强方法通过增加虚拟训练数据的大小和多样性来提高可学习模型的性能，而无需收集额外的数据样本。传统上，在图像识别任务中使用几何变换和颜色增强变换。例如，[17, 11] 随机应用水平翻转和裁剪，以及图像色调的交替变化。近年来，其他图像操作方法也被证明是有效的。[37, 6] 从图像中随机切出一个补丁，并用随机噪声或常数值替换它。另一种策略是通过凸组合[36, 29]或从它们中创建拼贴图[34]混合多个不同类别的图像。在这些研究中，操作的选择、它们的幅度以及应用的概率都是经过精心设计的。

### Automating Data Augmentation

&emsp;类似于 NAS，自动化数据增强是一个自然的方向。一种方向是使用黑盒优化技术搜索更好的符号操作组合：强化学习、进化策略、贝叶斯优化和基于种群的训练。作为目标，一些方法直接旨在最小化错误率，或者等效地最大化准确率，而另一些方法则试图匹配增强图像和原始图像的密度。


&emsp;另一个方向是使用生成对抗网络（GANs）。一些研究使用条件GANs生成有助于图像分类器性能的图像。另一些研究利用GANs修改模拟器的输出，使其看起来像真实对象。

* 自动化数据增强也可以应用于表示学习，例如半监督学习和域泛化。


## Preliminaries

* 在这一部分，我们描述了 AutoAugment、PBA 和 Fast AutoAugment 的共同基础（也可参见图 3）。Faster AutoAugment 也遵循这个问题设置。

* 在这些工作中，输入图像通过一个策略进行增强，该策略由 L 个不同的子策略 S 组成（l = 1, 2, ..., L）。随机选择的子策略会转换每个图像 X。单个子策略包括 K 个连续的图像处理操作 O1, O2, ..., OK，这些操作逐个应用于图像。我们将连续操作的数量 K 称为操作计数。在本文的其余部分，我们专注于子策略；因此，我们省略了上标 l。

* 每种方法首先搜索更好的策略。在搜索阶段结束后，获取的策略被用作数据增强管道来训练神经网络。

### 3.1. Operations

















