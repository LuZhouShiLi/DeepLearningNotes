# What Makes for Good Views for Contrastive Learning?


## 摘要
&emsp;这篇论文探讨了在自监督表示学习领域，多视图对比学习最近取得的突破性进展以及其在状态艺术性能中的地位。尽管对比学习取得了成功，但不同视图选择的影响却鲜少被研究。论文通过理论和实证分析，更好地理解了视图选择的重要性，并提出了一个观点：应该在保持任务相关信息完整的同时，降低视图之间的互信息（Mutual Information，MI）。

为了验证这一假设，作者设计了无监督和半监督框架，通过旨在降低其MI来学习有效的视图。此外，还考虑了数据增强作为降低MI的一种方法，并展示了增加数据增强确实可以导致MI减少，并提高下游分类准确率。作为一个附带的成果，作者在ImageNet分类的无监督预训练中达到了新的状态艺术准确率（使用ResNet-50达到了73%的top-1线性读出）。



## Introduction

* 这段文字深入探讨了在多视图学习中构建不变表示的重要性和挑战，以及如何平衡在不变性和任务相关性之间的关系。通过引用豪尔赫·路易斯·博尔赫斯（Jorge Luis Borges）的短篇小说《记忆力惊人的富内斯》（Funes the Memorious）中的一个例子，来说明对于每一个细微差异都能感知到的富内斯来说，无法将不同视角下看到的相同对象归为一类的困境。这个故事为理解多视图学习提供了一个独特的视角，特别是在决定哪些变化应该被模型忽略以提取不变特征时的重要性。

* 在多视图学习，尤其是对比多视图学习的背景下，这段讨论引出了一个核心问题：“我们应该对哪些观察条件保持不变？”这个问题突出了寻找平衡点的重要性：既不是对所有可能的变化都保持不变（这可能会导致丢失对任务来说重要的信息），也不是对每个细微的变化都进行独立表示（这会导致表示的过度碎片化，降低模型的泛化能力）。

* 对比多视图学习尝试通过将同一场景的两个视图在表示空间中拉近，将不同场景的两个视图推远，从而学习到能够概括不同视角下对象不变特征的表示。这种方法的核心在于，通过这种方式，模型学习到的表示能够抽象出场景的本质特征，忽略那些对完成特定任务无关紧要的变化（如时间、观察角度等）。

* 然而，正如这段讨论所强调的，决定哪些变化是应该被模型忽略的，哪些又是对完成特定任务至关重要的，是一个需要精心权衡的问题。这不仅关系到模型的泛化能力，也关系到它在特定任务上的性能。例如，如果任务是对时间进行分类，那么模型就不应该对时间变化保持不变；而在需要追踪场景中动态对象的任务中，模型又需要能够跨越不同的观察角度，识别出对象的连续性。

* 这段文字不仅提出了多视图学习中的一个关键挑战，也通过博尔赫斯的小说中的比喻，以一种非常生动的方式强调了这个挑战的复杂性和对于模型设计的影响。这为我们在设计和评估多视图学习模型时提供了重要的思考角度。


* 这段文字进一步阐述了在对比学习框架中如何通过选择“视图”来控制表示捕获的信息，以达到既不丢失下游任务所需信息又能对不重要的变异保持鲁棒性的平衡。这里的“视图”可以是不同的感官信号、图像通道、时间切片，或同一数据张量的不同“增强”版本。通过这种方式，表示将集中于视图之间共享的信息，从而实现对无关变量的不变性。

* 研究提出了“信息最小化（InfoMin）原则”，即好的视图集应该共享完成下游任务所必需的最小信息。这与最小充分统计量和信息瓶颈理论相联系，旨在通过抛弃对任务无关的信息来提高泛化能力和降低下游任务的样本复杂度。与此同时，这一原则与已经流行的“信息最大化（InfoMax）原则”形成互补，后者主张在表示学习中尽可能捕获关于刺激的信息，但我们强调，只有当这些信息与任务相关时，最大化信息才是有用的。

* 为了验证这一原则，作者以两种方式进行了探讨：一是展示了视图的最优选择严重依赖于下游任务，如果已知任务，则通常可以设计出有效的视图。二是通过实证表明，在生成视图的许多常见方式中，存在一个在下游性能方面的最佳点，即视图之间的互信息（MI）既不太高也不太低。

* 基于这些发现，研究还引入了一种半监督方法，用于在已知下游任务的情况下学习有效视图，以及通过寻求更强的数据增强来进一步降低互信息，从而应用InfoMin原则。这种努力在标准基准测试上取得了最新的准确率。

* 这些观点强调了在设计表示学习算法时，对于视图选择的重要性和精细度，以及如何通过理论和实证分析来指导这一选择，以优化模型对下游任务的性能。此外，通过引入半监督方法和利用数据增强来调整互信息，为实际应用中如何利用InfoMin原则提供了具体的策略。这不仅有助于提高模型的准确性，还能在一定程度上降低对标记数据的依赖，从而在复杂和多变的真实世界任务中具有更广泛的应用前景。




* 证明了对比表示学习的最优视图是依赖于任务的。这意味着为了实现高效的学习和良好的性能，选择视图时需要考虑下游任务的特性和要求。
* 在多种设置中，实证发现互信息估计与表示质量之间存在U形关系。这表明存在一个最佳的信息共享水平，既不是太多也不是太少，能够最大化表示的效用。
* 提出了一种新的半监督方法，用于给定任务学习有效的视图。这种方法允许在不完全依赖标签数据的情况下，根据任务的特定需求来调整视图生成策略，从而提高表示的质量。
* 将这些理解应用于实践，实现了在ImageNet线性读出基准测试上使用ResNet-50达到73.0%的最新准确率。这一成绩展示了通过优化视图选择和表示学习策略，可以显著提高模型在复杂视觉任务中的性能。

## Related Work


* 这段文字讨论了在无监督表示学习中，特别是自监督对比表示学习领域的最新进展。自监督对比学习通过对比损失函数来学习表示，该损失函数将不相似的数据对推开，同时将相似的数据对拉近，这一思想与示例学习相似。基于对比损失的模型显著优于其他方法。
* 利用标记数据在对比表示学习中已显示出引导表示学习向任务相关特征的能力，从而提高性能。在这里，我们使用标记数据来学习更好的视图，但仍然只使用未标记数据进行对比学习。未来的工作可能结合这些方法，利用标签同时进行视图学习和表示学习。此外，之前的工作研究了使用不同数量的图像进行增强的效果。
* 这一段强调了在自监督对比学习中选择和生成数据对的重要性，并展望了将标记数据与未标记数据结合使用的潜力，这既可以优化表示学习的过程，也能够为特定任务学习更加相关的特征。这些进展为进一步提高无监督学习方法的性能和应用提供了新的思路和机会。



## What Are the Optimal Views for Contrastive Learning?


### Multiview contrastive learning




