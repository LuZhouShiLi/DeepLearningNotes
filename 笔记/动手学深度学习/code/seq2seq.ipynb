{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  编码器\n",
    "\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    # 用于序列到序列学习的循环神经网络编码器\n",
    "\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqEncoder,self).__init__(**kwargs)\n",
    "\n",
    "        #  嵌入\n",
    "        # 将每一个单词转换为embed_szie个向量 单词数量是vocab_size\n",
    "        # 将一个单词转换成 50个维度的向量\n",
    "        self.embedding  = nn.Embedding(vocab_size,embed_size)\n",
    "\n",
    "        # RNN层  将embed_size 转换成 num_hiddens  比如50 -> 20 个特征\n",
    "        # num_layers是RNN的层\n",
    "        self.rnn = nn.GRU(embed_size,num_hiddens,num_layers,dropout=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 前向传播  X的形状是  batch_size num_steps, embed_size\n",
    "        X = self.embedding(X)\n",
    "\n",
    "        # 循环神经网络中 第一个轴对应于时间步 也就是多少个单词数量\n",
    "        # 举一个例子 比如X = torch.randn(10,3,100)\n",
    "        # 那么10就是代表 num_steps 也就是时间步  其实就是每一个句子的单词数量\n",
    "        # 3 代表有三个句子  100 代表每一个单词是100个特征进行表示\n",
    "        X = X.permute(1,0,2) # num_steps参数放在前面\n",
    "\n",
    "        # RNN正向传播  这里只传入向量  返回所有时间步的最后一层的隐藏状态\n",
    "        # state是最后一个时间步的所有层隐藏状态\n",
    "        # output的形状  num_steps,batchsize num_hidden\n",
    "        # state num_layers batchsize num_hidden\n",
    "        output,state = self.rnn(X)\n",
    "\n",
    "        return output,state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10,embed_size=8,num_hiddens=16,num_layers=2)\n",
    "\n",
    "encoder.eval()\n",
    "#  四条句子 每个句子有七个单词\n",
    "X = torch.zeros((4,7),dtype=torch.long)\n",
    "\n",
    "# 输出形状 7 x 4 x 16  7代表 时间步 也就是七个单词  4代表 四条句子  16代表被提取的维度\n",
    "output,state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    def __init(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqDecoder,self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "        # 嵌入\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "\n",
    "        # RNN  \n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens,num_hiddens,num_layers,dropout=dropout)\n",
    "\n",
    "        # 输出层\n",
    "        self.dense = nn.Linear(num_hiddens,vocab_size)\n",
    "\n",
    "    # encoder的最后被一个时间步的state 作为Deconde初始化的state\n",
    "    # \n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).permute(1,0,2)\n",
    "\n",
    "        # state[-1] 作为最后一层的隐藏状态  形状1 batch_size encoder的 num_hiddens\n",
    "        # state[-1].repeat(X.shape[0],1,1) 广播操作 变成num_step batch_size num_hiddens\n",
    "        context = state[-1].repeat(X.shape[0],1,1)\n",
    "\n",
    "        # 将输入的X 和encoder最后一步的最后一层的隐藏转台拼接在一起  作为decoder的输入\n",
    "        # 输出形状为(num_steps，batch_size，embed_size + encoder的num_hiddens)\n",
    "        X_and_context = torch.cat((X,context),2)\n",
    "\n",
    "        # 输出形状是 num_steps batche_size num_hiddens\n",
    "        output = self.dense(output).permute(1,0,2)\n",
    "\n",
    "        return output,state\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
