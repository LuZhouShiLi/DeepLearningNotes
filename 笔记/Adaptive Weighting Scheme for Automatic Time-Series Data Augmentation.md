# Adaptive Weighting Scheme for Automatic Time-Series Data Augmentation


## 摘要


该段落讨论了数据增强方法在图像、文本和音频分类任务中提高泛化能力的重要性，并指出自动化增强方法最近在图像分类和对象检测领域取得了进一步的改进，达到了最先进的性能。然而，对时间序列数据的自动化数据增强工作却相对较少，这是一个可能极大受益于自动化数据增强的领域，尤其是鉴于这类数据集通常较小。

作者提出了两种针对数据增强的样本自适应自动权重方案：

**第一种方案学习加权增强样本对损失的贡献**
**第二种方法基于预测的训练损失排名选择一组变换(transformation)。**



## Introduction

* 这一段落进一步阐述了数据增强作为一种主流方法，在减少神经网络过拟合和提高泛化能力方面的重要性。它概述了数据增强在图像分类、语音识别和自然语言处理等领域已经取得了显著的改进效果，并提到了它在时间序列分类任务和金融预测中的应用。这强调了数据增强在各种机器学习应用中的普遍性和效用。

* 然而，文章也指出了一个主要的挑战：如何在可能的变换空间中搜索，以找到可以应用于某一类样本的变换，这些变换能够生成具有同一类特征的增强样本。这是由于可能的变换及其相关参数数量庞大，使得这个问题变得非常复杂。比如，给样本添加噪声时应该使用什么规模的噪声？哪些变换在特定数据集上效果最好？如何执行可学习的数据增强仍然是一个开放的问题。

* 为了解决这一问题，论文提出了两种新颖的自动化增强策略，专门用于时间序列数据，并将其与RandAugment（Cubuk等，2020年）进行了比较——这是首次将RandAugment应用于时间序列数据。所提出的策略通过在训练过程中结合多种增强方式来工作，要么是根据它们的损失选择增强样本，要么是通过使用一个可训练的权重向量对每个增强样本的损失贡献进行加权，该权重向量与神经网络的参数同时训练。此外，文章还建议引入一个独特的超参数，通过对每种增强的所有超参数进行分组，来调节增强的强度。

* 这一研究的创新之处在于它尝试通过自动化的方法解决数据增强中的优化问题，尤其是针对时间序列数据，这在以往的研究中鲜有涉及。通过这种方法，可以更加有效和精确地应用数据增强，以提高模型在特定任务上的表现。

## Related Work


* 这一段落讨论了自动化数据增强方法的发展，特别是在寻找不同变换组合的最优策略方面的兴趣增长。文章首先提到了一些早期尝试，比如使用生成对抗网络方法的TANDA（Transformation Adversarial Networks for Data Augmentations），以及利用强化学习框架自动搜索改进数据增强策略的AutoAugment和Adversarial AutoAugment。这些方法通过不同的技术途径试图找到最优的数据增强策略，以提高模型在标准数据集上的性能。

* 接着，文章介绍了RandAugment，这是一种更简单的方法，它在训练过程中对每个小批量数据随机应用一组变换，减少了搜索空间，并通过简单的网格搜索来优化参数。RandAugment在CIFAR、SVHN和ImageNet基准测试上达到或超过了以前所有增强方法的表现。

* 然而，这些方法通常学习一个对所有训练样本不变的策略。与此相反，本文提出的方法学习一个样本自适应策略，这种策略学会权衡个别变换的贡献，或者基于样本的损失选择最相关的变换。此外，我们的方法是第一种在时间序列数据上实施自动化数据增强策略的方法。与Wu等人提出的基于不确定性的随机采样方案不同，我们的策略是样本自适应的；我们对每个样本应用所有增强，并自动调整每个增强及原始样本的贡献，或者基于它们的损失选择一部分增强。我们观察到，保持损失最高的增强样本在使用增强样本集合对网络参数进行更新时，会产生负面效果。

* 本文的方法通过提供一种自适应和灵活的增强策略，解决了以往方法中的一些局限性，尤其是在处理时间序列数据时。通过考虑每个样本的特定情况和损失，该方法更有可能找到更适合特定数据集的最优变换组合，进而提高模型的性能和鲁棒性。


## Augmentation policies

### W-Augment
W-Augment方法对每个小批量中的样本应用所有N种增强，并计算每个增强样本的交叉熵损失。不同之处在于，它不是简单地平均所有样本的损失**，而是通过一个可训练的权重向量ω（维度为N+1）来加权每个样本的损失**。权重向量的每个元素都通过softmax函数进行归一化，确保它们的和为一。**这个加权损失后用于反向传播更新网络参数**。该方法的关键在于通过学习权重向量来自动调整每种增强的贡献，从而优化模型性能。


- **方法名称**: W-Augment
- **增强应用**: 对每个样本应用所有\(N\)种增强。
- **损失计算**: 计算每个增强样本的交叉熵损失，不进行聚合。
- **权重应用**: 使用点积和可训练的权重向量\(\omega\)（通过softmax归一化）加权损失。
- **目的**: 通过自适应权重调整各增强方法的贡献，优化模型训练。

### α-trimmed Augment


α-trimmed Augment方法也是对每个样本应用所有N种变换，并计算每个变换的损失。不同的是，它通过排名损失并丢弃最高和最低的α个损失来选择损失中等的增强样本进行训练。这种方法的目的是排除那些对模型训练帮助不大（损失太小）或者过于困难（损失太大）的样本，以达到更平衡的训练效果。

- **方法名称**: α-trimmed Augment
- **增强应用**: 对每个样本应用所有\(N\)种增强。
- **损失计算**: 单独计算每个增强样本的损失，并进行排名。
- **样本选择**: 丢弃最高和最低的\(\alpha\)个损失的增强样本，只使用中等损失的样本进行训练。
- **目的**: 排除对训练帮助不大或过于困难的样本，实现更有效的模型训练。



