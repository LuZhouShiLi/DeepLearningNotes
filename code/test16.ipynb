{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyConvNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool2): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#  定义自编码器网络模型\n",
    "#  压缩比  15 \n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self, compression_ratio):\n",
    "        super(MyConvNet, self).__init__()\n",
    "        #  输入1  输出 16  15000 x 1 -> 15000 x 16\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1) # 1D convolutional layer with 16 filters\n",
    "        #  15000 x 16 -> 5000 x 16\n",
    "        self.maxpool1 = nn.MaxPool2d(3) # Max pooling layer with compression_ratio stride\n",
    "       \n",
    "        # 5000 x 16  -> 5000 x 32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) # 1D convolutional layer with 64 filters\n",
    "\n",
    "        #  5000 x 32 -> 5000 x 64\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 1D convolutional layer with 64 filters\n",
    "\n",
    "        # 5000 x 64-> 1000 x 64\n",
    "        self.maxpool2 = nn.MaxPool2d(5)\n",
    "\n",
    "        # 1000 x 64 -> 1000 x 128\n",
    "        self.conv4 = nn.Conv2d(64,128,kernel_size=3,padding=1)\n",
    "\n",
    "        # 最后一层卷积 1000 x 128 -> 1000 x 1\n",
    "        self.conv5 = nn.Conv2d(128,1,kernel_size=3,padding=1)\n",
    "\n",
    "    # encoder forward 前向传播\n",
    "    def forward(self, x):\n",
    "        alpha = 0.1\n",
    "        elu = nn.ELU(alpha=alpha)  # 创建elu激活函数\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = elu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = elu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = elu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#  定义网络模型\n",
    "# Instantiate the model with a compression ratio of 15   15000 -> 1000\n",
    "compression_ratio = 15\n",
    "model = MyConvNet(compression_ratio)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  定义损失函数  均方根\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#  学习率 \n",
    "learning_rate = 0.002\n",
    "\n",
    "#  定义优化器  随机梯度下降\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地震道数量:42\n",
      "采样点数量：15000\n",
      "Seismic data shape: (42, 15000)\n",
      "Trace 0 data: [-0.02413988 -0.02503395 -0.02622605 -0.02384186 -0.02413988 -0.01996756\n",
      " -0.02026558 -0.02384186 -0.01996756 -0.01817942]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "import numpy as np\n",
    "import segyio\n",
    "\n",
    "# 读取SEGY文件\n",
    "segy_file = '../SegyData/20220101_115300.sgy'\n",
    "\n",
    "with segyio.open(segy_file, 'rb') as segy:\n",
    "    # 获取地震数据的一维数组\n",
    "    seismic_data = segy.trace.raw[:]\n",
    "\n",
    "    # 获取地震道数量和每个地震道的时间采样点数量\n",
    "    n_traces = segy.tracecount\n",
    "    n_samples = segy.samples.size\n",
    "\n",
    "    print(\"地震道数量:{}\".format(n_traces))\n",
    "    print(\"采样点数量：{}\".format(n_samples))\n",
    "\n",
    "    # 将一维数组重新形状为二维数组，形状为 (n_traces, n_samples)\n",
    "    seismic_data = seismic_data.reshape((n_traces, n_samples))\n",
    "\n",
    "# 打印地震数据的形状  42 x 15000  42个地震道  15000个采样点\n",
    "print(\"Seismic data shape:\", seismic_data.shape)\n",
    "\n",
    "# 可以使用NumPy进行进一步的处理和分析\n",
    "# 例如，获取第一条地震道的前10个采样点\n",
    "trace_0 = seismic_data[0, :10]\n",
    "print(\"Trace 0 data:\", trace_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地震道数量:42\n",
      "采样点数量：15000\n",
      "训练集大小: torch.Size([33, 15000])\n",
      "测试集大小: torch.Size([9, 15000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 处理数据集\n",
    "# 如果数据是 594 x 1500 将数据处理成 594  x 1 x 1500 按照batch_size 分批\n",
    "\n",
    "import segyio\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 读取SEGY文件\n",
    "segy_file = '../SegyData/20220101_115300.sgy'\n",
    "\n",
    "with segyio.open(segy_file, 'rb') as segy:\n",
    "    # 获取地震数据的一维数组\n",
    "    seismic_data = segy.trace.raw[:]\n",
    "\n",
    "    # 获取地震道数量和每个地震道的时间采样点数量\n",
    "    n_traces = segy.tracecount\n",
    "    n_samples = segy.samples.size\n",
    "\n",
    "    print(\"地震道数量:{}\".format(n_traces))\n",
    "    print(\"采样点数量：{}\".format(n_samples))\n",
    "\n",
    "    # 将一维数组重新形状为二维数组，形状为 (n_traces, n_samples)\n",
    "    seismic_data = seismic_data.reshape((n_traces, n_samples))\n",
    "\n",
    "# 随机划分为训练集和测试集\n",
    "train_data, test_data = train_test_split(seismic_data, test_size=0.2, random_state=42)\n",
    "#  分割成 训练集和测试集\n",
    "# 将数据转换为PyTorch张量  \n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "print(f\"训练集大小: {train_data.shape}\")\n",
    "print(f\"测试集大小: {test_data.shape}\")\n",
    "\n",
    "\n",
    "# reshape 42 x 15000 x 1\n",
    "train_data = train_data.reshape(33,15000,1)\n",
    "test_data = test_data.reshape(9,15000,1)\n",
    "\n",
    "#  数据预处理  归一化 0，1 之间\n",
    "\n",
    "# 上面的数据是 42 x 15000 然后reshape 42 x 1 x 15000\n",
    "\n",
    "#  dataloader加载数据集\n",
    "train_dataloader = DataLoader(train_data,batch_size = 3)\n",
    "test_dataloader = DataLoader(test_data,batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  设置训练网络的一些参数\n",
    "total_train_step = 0\n",
    "# 记录测试的次数\n",
    "total_test_step = 0\n",
    "# 训练的轮数\n",
    "epoch = 200\n",
    "# writer  = SummaryWriter(\"../encoder_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 定义自编码器网络\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "\n",
    "        #  编码器 最后输出 1000 x 1 的结果\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1) ,# 1D convolutional layer with 16 filters\n",
    "\n",
    "            #  15000 x 16 -> 5000 x 16\n",
    "            nn.MaxPool2d(3) ,# Max pooling layer with compression_ratio stride\n",
    "       \n",
    "            # 5000 x 16  -> 5000 x 32\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1) ,# 1D convolutional layer with 64 filters\n",
    "\n",
    "            #  5000 x 32 -> 5000 x 64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1) ,# 1D convolutional layer with 64 filters\n",
    "\n",
    "            # 5000 x 64-> 1000 x 64\n",
    "            nn.MaxPool2d(5),\n",
    "\n",
    "            # 1000 x 64 -> 1000 x 128\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "\n",
    "            # 最后一层卷积 1000 x 128 -> 1000 x 1\n",
    "            nn.Conv2d(128,1,kernel_size=3,padding=1)\n",
    "        )\n",
    "\n",
    "        # 解码器网络\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 100 x 1 -> 100 x 128\n",
    "            nn.ConvTranspose2d(1,128,kernel_size=3,padding=1),\n",
    "\n",
    "            # 100 x 128 -> 500 x 128\n",
    "            nn.Upsample(scale_factor=5),\n",
    "\n",
    "            #  500 x 128 -> 500 x 64\n",
    "            nn.ConvTranspose2d(128,64,kernel_size=3,padding=1),\n",
    "\n",
    "            # 500 x 64 -> 500 x 32\n",
    "            nn.ConvTranspose2d(64,32,kernel_size=3,padding=1),\n",
    "            \n",
    "            # 500 x 32 -> 1500 x 32\n",
    "            nn.Upsample(scale_factor=3),\n",
    "\n",
    "            #  1500 x 32 -> 1500 x 16\n",
    "            nn.ConvTranspose2d(32,16,kernel_size=3,padding=1),\n",
    "\n",
    "            #  1500 x 16 -> 1500 x 1\n",
    "            nn.ConvTranspose2d(16,1,kernel_size=3,padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 15000)\n",
      "1\n",
      "(42, 15000)\n",
      "2\n",
      "(42, 15000)\n",
      "3\n",
      "(42, 15000)\n",
      "4\n",
      "(42, 15000)\n",
      "5\n",
      "(42, 15000)\n",
      "6\n",
      "(42, 15000)\n",
      "7\n",
      "(42, 15000)\n",
      "8\n",
      "(42, 15000)\n",
      "9\n",
      "(42, 15000)\n",
      "10\n",
      "(42, 15000)\n",
      "11\n",
      "(42, 15000)\n",
      "12\n",
      "(42, 15000)\n",
      "13\n",
      "(42, 15000)\n",
      "14\n",
      "(42, 15000)\n",
      "15\n",
      "(42, 15000)\n",
      "16\n",
      "(42, 15000)\n",
      "17\n",
      "(42, 15000)\n",
      "18\n",
      "(42, 15000)\n",
      "19\n",
      "(42, 15000)\n",
      "20\n",
      "(42, 15000)\n",
      "21\n",
      "(42, 15000)\n",
      "22\n",
      "(42, 15000)\n",
      "23\n",
      "(42, 15000)\n",
      "24\n",
      "(42, 15000)\n",
      "25\n",
      "(42, 15000)\n",
      "26\n",
      "(42, 15000)\n",
      "27\n",
      "(42, 15000)\n",
      "28\n",
      "(42, 15000)\n",
      "29\n",
      "(42, 15000)\n",
      "30\n",
      "(42, 15000)\n",
      "31\n",
      "(42, 15000)\n",
      "32\n",
      "(42, 15000)\n",
      "33\n",
      "(42, 15000)\n",
      "34\n",
      "(42, 15000)\n",
      "35\n",
      "(42, 15000)\n",
      "36\n",
      "(42, 15000)\n",
      "37\n",
      "(42, 15000)\n",
      "38\n",
      "(42, 15000)\n",
      "39\n",
      "(42, 15000)\n",
      "40\n",
      "(42, 15000)\n",
      "41\n",
      "(42, 15000)\n",
      "42\n",
      "(42, 15000)\n",
      "43\n",
      "(42, 15000)\n",
      "44\n",
      "(42, 15000)\n",
      "45\n",
      "(42, 15000)\n",
      "46\n",
      "(42, 15000)\n",
      "47\n",
      "(42, 15000)\n",
      "48\n",
      "(42, 15000)\n",
      "49\n",
      "(42, 15000)\n",
      "50\n",
      "(42, 15000)\n",
      "51\n",
      "(42, 15000)\n",
      "52\n",
      "(42, 15000)\n",
      "53\n",
      "(42, 15000)\n",
      "54\n",
      "(42, 15000)\n",
      "55\n",
      "(42, 15000)\n",
      "56\n",
      "(42, 15000)\n",
      "57\n",
      "(42, 15000)\n",
      "58\n",
      "(42, 15000)\n",
      "59\n",
      "(42, 15000)\n",
      "60\n",
      "(42, 15000)\n",
      "61\n",
      "(42, 15000)\n",
      "62\n",
      "(42, 15000)\n",
      "63\n",
      "(42, 15000)\n",
      "64\n",
      "(42, 15000)\n",
      "65\n",
      "(42, 15000)\n",
      "66\n",
      "(42, 15000)\n",
      "67\n",
      "(42, 15000)\n",
      "68\n",
      "(42, 15000)\n",
      "69\n",
      "(42, 15000)\n",
      "70\n",
      "(42, 15000)\n",
      "71\n",
      "(42, 15000)\n",
      "72\n",
      "(42, 15000)\n",
      "73\n",
      "(42, 15000)\n",
      "74\n",
      "(42, 15000)\n",
      "75\n",
      "(42, 15000)\n",
      "76\n",
      "(42, 15000)\n",
      "77\n",
      "(42, 15000)\n",
      "78\n",
      "(42, 15000)\n",
      "79\n",
      "(42, 15000)\n",
      "80\n",
      "(42, 15000)\n",
      "81\n",
      "(42, 15000)\n",
      "82\n",
      "(42, 15000)\n",
      "83\n",
      "(42, 15000)\n",
      "84\n",
      "(42, 15000)\n",
      "85\n",
      "(42, 15000)\n",
      "86\n",
      "(42, 15000)\n",
      "87\n",
      "(42, 15000)\n",
      "88\n",
      "(42, 15000)\n",
      "89\n",
      "(42, 15000)\n",
      "90\n",
      "(42, 15000)\n",
      "91\n",
      "(42, 15000)\n",
      "92\n",
      "(42, 15000)\n",
      "93\n",
      "(42, 15000)\n",
      "94\n",
      "(42, 15000)\n",
      "95\n",
      "(42, 15000)\n",
      "96\n",
      "(42, 15000)\n",
      "97\n",
      "(42, 15000)\n",
      "98\n",
      "(42, 15000)\n",
      "99\n",
      "(42, 15000)\n",
      "100\n",
      "(42, 15000)\n",
      "101\n",
      "(42, 15000)\n",
      "102\n",
      "(42, 15000)\n",
      "103\n",
      "(42, 15000)\n",
      "104\n",
      "(42, 15000)\n",
      "105\n",
      "(42, 15000)\n",
      "106\n",
      "(42, 15000)\n",
      "107\n",
      "(42, 15000)\n",
      "108\n",
      "(42, 15000)\n",
      "109\n",
      "(42, 15000)\n",
      "110\n",
      "(42, 15000)\n",
      "111\n",
      "(42, 15000)\n",
      "112\n",
      "(42, 15000)\n",
      "113\n",
      "(42, 15000)\n",
      "114\n",
      "(42, 15000)\n",
      "115\n",
      "(42, 15000)\n",
      "116\n",
      "(42, 15000)\n",
      "117\n",
      "(42, 15000)\n",
      "118\n",
      "(42, 15000)\n",
      "119\n",
      "(42, 15000)\n",
      "120\n",
      "(42, 15000)\n",
      "121\n",
      "(42, 15000)\n",
      "122\n",
      "(42, 15000)\n",
      "123\n",
      "(42, 15000)\n",
      "124\n",
      "(42, 15000)\n",
      "125\n",
      "(42, 15000)\n",
      "126\n",
      "(42, 15000)\n",
      "127\n",
      "(42, 15000)\n",
      "128\n",
      "(42, 15000)\n",
      "129\n",
      "(42, 15000)\n",
      "130\n",
      "(42, 15000)\n",
      "131\n",
      "(42, 15000)\n",
      "132\n",
      "(42, 15000)\n",
      "133\n",
      "(42, 15000)\n",
      "134\n",
      "(42, 15000)\n",
      "135\n",
      "(42, 15000)\n",
      "136\n",
      "(42, 15000)\n",
      "137\n",
      "(42, 15000)\n",
      "138\n",
      "(42, 15000)\n",
      "139\n",
      "(42, 15000)\n",
      "140\n",
      "(42, 15000)\n",
      "141\n",
      "(42, 15000)\n",
      "142\n",
      "(42, 15000)\n",
      "143\n",
      "(42, 15000)\n",
      "144\n",
      "(42, 15000)\n",
      "145\n",
      "(42, 15000)\n",
      "146\n",
      "(42, 15000)\n",
      "147\n",
      "(42, 15000)\n",
      "148\n",
      "(42, 15000)\n",
      "149\n",
      "(42, 15000)\n",
      "150\n",
      "(42, 15000)\n",
      "151\n",
      "(42, 15000)\n",
      "152\n",
      "(42, 15000)\n",
      "153\n",
      "(42, 15000)\n",
      "154\n",
      "(42, 15000)\n",
      "155\n",
      "(42, 15000)\n",
      "156\n",
      "(42, 15000)\n",
      "157\n",
      "(42, 15000)\n",
      "158\n",
      "(42, 15000)\n",
      "159\n",
      "(42, 15000)\n",
      "160\n",
      "(42, 15000)\n",
      "161\n",
      "(42, 15000)\n",
      "162\n",
      "(42, 15000)\n",
      "163\n",
      "(42, 15000)\n",
      "164\n",
      "(42, 15000)\n",
      "165\n",
      "(42, 15000)\n",
      "166\n",
      "(42, 15000)\n",
      "167\n",
      "(42, 15000)\n",
      "168\n",
      "(42, 15000)\n",
      "169\n",
      "(42, 15000)\n",
      "170\n",
      "(42, 15000)\n",
      "171\n",
      "(42, 15000)\n",
      "172\n",
      "(42, 15000)\n",
      "173\n",
      "(42, 15000)\n",
      "174\n",
      "(42, 15000)\n",
      "175\n",
      "(42, 15000)\n",
      "176\n",
      "(42, 15000)\n",
      "177\n",
      "(42, 15000)\n",
      "178\n",
      "(42, 15000)\n",
      "179\n",
      "(42, 15000)\n",
      "180\n",
      "(42, 15000)\n",
      "181\n",
      "(42, 15000)\n",
      "182\n",
      "(42, 15000)\n",
      "183\n",
      "(42, 15000)\n",
      "184\n",
      "(42, 15000)\n",
      "185\n",
      "(42, 15000)\n",
      "186\n",
      "(42, 15000)\n",
      "187\n",
      "(42, 15000)\n",
      "188\n",
      "(42, 15000)\n",
      "189\n",
      "(42, 15000)\n",
      "190\n",
      "(42, 15000)\n",
      "191\n",
      "(42, 15000)\n",
      "192\n",
      "(42, 15000)\n",
      "193\n",
      "(42, 15000)\n",
      "194\n",
      "(42, 15000)\n",
      "195\n",
      "(42, 15000)\n",
      "196\n",
      "(42, 15000)\n",
      "197\n",
      "(42, 15000)\n",
      "198\n",
      "(42, 15000)\n",
      "199\n",
      "(42, 15000)\n",
      "200\n",
      "(42, 15000)\n",
      "201\n",
      "(42, 15000)\n",
      "202\n",
      "(42, 15000)\n",
      "203\n",
      "(42, 15000)\n",
      "204\n",
      "(42, 15000)\n",
      "205\n",
      "(42, 15000)\n",
      "206\n",
      "(42, 15000)\n",
      "207\n",
      "(42, 15000)\n",
      "208\n",
      "(42, 15000)\n",
      "209\n",
      "(42, 15000)\n",
      "210\n",
      "(42, 15000)\n",
      "211\n",
      "(42, 15000)\n",
      "212\n",
      "(42, 15000)\n",
      "213\n",
      "(42, 15000)\n",
      "214\n",
      "(42, 15000)\n",
      "215\n",
      "(42, 15000)\n",
      "216\n",
      "(42, 15000)\n",
      "217\n",
      "(42, 15000)\n",
      "218\n",
      "(42, 15000)\n",
      "219\n",
      "(42, 15000)\n",
      "220\n",
      "(42, 15000)\n",
      "221\n",
      "(42, 15000)\n",
      "222\n",
      "(42, 15000)\n",
      "223\n",
      "(42, 15000)\n",
      "224\n",
      "(42, 15000)\n",
      "225\n",
      "(42, 15000)\n",
      "226\n",
      "(42, 15000)\n",
      "227\n",
      "(42, 15000)\n",
      "228\n",
      "(42, 15000)\n",
      "229\n",
      "(42, 15000)\n",
      "230\n",
      "(42, 15000)\n",
      "231\n",
      "(42, 15000)\n",
      "232\n",
      "(42, 15000)\n",
      "233\n",
      "(42, 15000)\n",
      "234\n",
      "(42, 15000)\n",
      "235\n",
      "(42, 15000)\n",
      "236\n",
      "(42, 15000)\n",
      "237\n",
      "(42, 15000)\n",
      "238\n",
      "(42, 15000)\n",
      "239\n",
      "(42, 15000)\n",
      "240\n",
      "(42, 15000)\n",
      "241\n",
      "(42, 15000)\n",
      "242\n",
      "(42, 15000)\n",
      "243\n",
      "(42, 15000)\n",
      "244\n",
      "(42, 15000)\n",
      "245\n",
      "(42, 15000)\n",
      "246\n",
      "(42, 15000)\n",
      "247\n",
      "(42, 15000)\n",
      "248\n",
      "(42, 15000)\n",
      "249\n",
      "(42, 15000)\n",
      "250\n",
      "(42, 15000)\n",
      "251\n",
      "(42, 15000)\n",
      "252\n",
      "(42, 15000)\n",
      "253\n",
      "(42, 15000)\n",
      "254\n",
      "(42, 15000)\n",
      "255\n",
      "(42, 15000)\n",
      "256\n",
      "(42, 15000)\n",
      "257\n",
      "(42, 15000)\n",
      "258\n",
      "(42, 15000)\n",
      "259\n",
      "(42, 15000)\n",
      "260\n",
      "(42, 15000)\n",
      "261\n",
      "(42, 15000)\n",
      "262\n",
      "(42, 15000)\n",
      "263\n",
      "(42, 15000)\n",
      "264\n",
      "(42, 15000)\n",
      "265\n",
      "(42, 15000)\n",
      "266\n",
      "(42, 15000)\n",
      "267\n",
      "(42, 15000)\n",
      "268\n",
      "(42, 15000)\n",
      "269\n",
      "(42, 15000)\n",
      "270\n",
      "(42, 15000)\n",
      "271\n",
      "(42, 15000)\n",
      "272\n",
      "(42, 15000)\n",
      "273\n",
      "(42, 15000)\n",
      "274\n",
      "(42, 15000)\n",
      "275\n",
      "(42, 15000)\n",
      "276\n",
      "(42, 15000)\n",
      "277\n",
      "(42, 15000)\n",
      "278\n",
      "(42, 15000)\n",
      "279\n",
      "(42, 15000)\n",
      "280\n",
      "(42, 15000)\n",
      "281\n",
      "(42, 15000)\n",
      "282\n",
      "(42, 15000)\n",
      "283\n",
      "(42, 15000)\n",
      "284\n",
      "(42, 15000)\n",
      "285\n",
      "(42, 15000)\n",
      "286\n",
      "(42, 15000)\n",
      "287\n",
      "(42, 15000)\n",
      "288\n",
      "(42, 15000)\n",
      "289\n",
      "(42, 15000)\n",
      "290\n",
      "(42, 15000)\n",
      "291\n",
      "(42, 15000)\n",
      "292\n",
      "(42, 15000)\n",
      "293\n",
      "(42, 15000)\n",
      "294\n",
      "(42, 15000)\n",
      "295\n",
      "(42, 15000)\n",
      "296\n",
      "(42, 15000)\n",
      "297\n",
      "(42, 15000)\n",
      "298\n",
      "(42, 15000)\n",
      "299\n",
      "(42, 15000)\n",
      "300\n",
      "(42, 15000)\n",
      "301\n",
      "(42, 15000)\n",
      "302\n",
      "(42, 15000)\n",
      "303\n",
      "(42, 15000)\n",
      "304\n",
      "(42, 15000)\n",
      "305\n",
      "(42, 15000)\n",
      "306\n",
      "(42, 15000)\n",
      "307\n",
      "(42, 15000)\n",
      "308\n",
      "(42, 15000)\n",
      "309\n",
      "(42, 15000)\n",
      "310\n",
      "(42, 15000)\n",
      "311\n",
      "(42, 15000)\n",
      "312\n",
      "(42, 15000)\n",
      "313\n",
      "(42, 15000)\n",
      "314\n",
      "(42, 15000)\n",
      "315\n",
      "(42, 15000)\n",
      "316\n",
      "(42, 15000)\n",
      "317\n",
      "(42, 15000)\n",
      "318\n",
      "(42, 15000)\n",
      "319\n",
      "(42, 15000)\n",
      "320\n",
      "(42, 15000)\n",
      "321\n",
      "(42, 15000)\n",
      "322\n",
      "(42, 15000)\n",
      "323\n",
      "(42, 15000)\n",
      "324\n",
      "(42, 15000)\n",
      "325\n",
      "(42, 15000)\n",
      "326\n",
      "(42, 15000)\n",
      "327\n",
      "(42, 15000)\n",
      "328\n",
      "(42, 15000)\n",
      "329\n",
      "(42, 15000)\n",
      "330\n",
      "(42, 15000)\n",
      "331\n",
      "(42, 15000)\n",
      "332\n",
      "(42, 15000)\n",
      "333\n",
      "(42, 15000)\n",
      "334\n",
      "(42, 15000)\n",
      "335\n",
      "(42, 15000)\n",
      "336\n",
      "(42, 15000)\n",
      "337\n",
      "(42, 15000)\n",
      "338\n",
      "(42, 15000)\n",
      "339\n",
      "(42, 15000)\n",
      "340\n",
      "(42, 15000)\n",
      "341\n",
      "(42, 15000)\n",
      "342\n",
      "(42, 15000)\n",
      "343\n",
      "(42, 15000)\n",
      "344\n",
      "(42, 15000)\n",
      "345\n",
      "(42, 15000)\n",
      "346\n",
      "(42, 15000)\n",
      "347\n",
      "(42, 15000)\n",
      "348\n",
      "(42, 15000)\n",
      "349\n",
      "(42, 15000)\n",
      "350\n",
      "(42, 15000)\n",
      "351\n",
      "(42, 15000)\n",
      "352\n",
      "(42, 15000)\n",
      "353\n",
      "(42, 15000)\n",
      "354\n",
      "(42, 15000)\n",
      "355\n",
      "(42, 15000)\n",
      "356\n",
      "(42, 15000)\n",
      "357\n",
      "(42, 15000)\n",
      "358\n",
      "(42, 15000)\n",
      "359\n",
      "(42, 15000)\n",
      "360\n",
      "(42, 15000)\n",
      "361\n",
      "(42, 15000)\n",
      "362\n",
      "(42, 15000)\n",
      "363\n",
      "(42, 15000)\n",
      "364\n",
      "(42, 15000)\n",
      "365\n",
      "(42, 15000)\n",
      "366\n",
      "(42, 15000)\n",
      "367\n",
      "(42, 15000)\n",
      "368\n",
      "(42, 15000)\n",
      "369\n",
      "(42, 15000)\n",
      "370\n",
      "(42, 15000)\n",
      "371\n",
      "(42, 15000)\n",
      "372\n",
      "(42, 15000)\n",
      "373\n",
      "(42, 15000)\n",
      "374\n",
      "(42, 15000)\n",
      "375\n",
      "(42, 15000)\n",
      "376\n",
      "(42, 15000)\n",
      "377\n",
      "(42, 15000)\n",
      "378\n",
      "(42, 15000)\n",
      "379\n",
      "(42, 15000)\n",
      "380\n",
      "(42, 15000)\n",
      "381\n",
      "(42, 15000)\n",
      "382\n",
      "(42, 15000)\n",
      "383\n",
      "(42, 15000)\n",
      "384\n",
      "(42, 15000)\n",
      "385\n",
      "(42, 15000)\n",
      "386\n",
      "(42, 15000)\n",
      "387\n",
      "(42, 15000)\n",
      "388\n",
      "(42, 15000)\n",
      "389\n",
      "(42, 15000)\n",
      "390\n",
      "(42, 15000)\n",
      "391\n",
      "(42, 15000)\n",
      "392\n",
      "(42, 15000)\n",
      "393\n",
      "(42, 15000)\n",
      "394\n",
      "(42, 15000)\n",
      "395\n",
      "(42, 15000)\n",
      "396\n",
      "(42, 15000)\n",
      "397\n",
      "(42, 15000)\n",
      "398\n",
      "(42, 15000)\n",
      "399\n",
      "(42, 15000)\n",
      "400\n",
      "(42, 15000)\n",
      "401\n",
      "(42, 15000)\n",
      "402\n",
      "(42, 15000)\n",
      "403\n",
      "(42, 15000)\n",
      "404\n",
      "(42, 15000)\n",
      "405\n",
      "(42, 15000)\n",
      "406\n",
      "(42, 15000)\n",
      "407\n",
      "(42, 15000)\n",
      "408\n",
      "(42, 15000)\n",
      "409\n",
      "(42, 15000)\n",
      "410\n",
      "(42, 15000)\n",
      "411\n",
      "(42, 15000)\n",
      "412\n",
      "(42, 15000)\n",
      "413\n",
      "(42, 15000)\n",
      "414\n",
      "(42, 15000)\n",
      "415\n",
      "(42, 15000)\n",
      "416\n",
      "(42, 15000)\n",
      "417\n",
      "(42, 15000)\n",
      "418\n",
      "(42, 15000)\n",
      "419\n",
      "(42, 15000)\n",
      "420\n",
      "(42, 15000)\n",
      "421\n",
      "(42, 15000)\n",
      "422\n",
      "(42, 15000)\n",
      "423\n",
      "(42, 15000)\n",
      "424\n",
      "(42, 15000)\n",
      "425\n",
      "(42, 15000)\n",
      "426\n",
      "(42, 15000)\n",
      "427\n",
      "(42, 15000)\n",
      "428\n",
      "(42, 15000)\n",
      "429\n",
      "(42, 15000)\n",
      "430\n",
      "(42, 15000)\n",
      "431\n",
      "(42, 15000)\n",
      "432\n",
      "(42, 15000)\n",
      "433\n",
      "(42, 15000)\n",
      "434\n",
      "(42, 15000)\n",
      "435\n",
      "(42, 15000)\n",
      "436\n",
      "(42, 15000)\n",
      "437\n",
      "(42, 15000)\n",
      "438\n",
      "(42, 15000)\n",
      "439\n",
      "(42, 15000)\n",
      "440\n",
      "(42, 15000)\n",
      "441\n",
      "(42, 15000)\n",
      "442\n",
      "(42, 15000)\n",
      "443\n",
      "(42, 15000)\n",
      "444\n",
      "(42, 15000)\n",
      "445\n",
      "(42, 15000)\n",
      "446\n",
      "(42, 15000)\n",
      "447\n",
      "(42, 15000)\n",
      "448\n",
      "(42, 15000)\n",
      "449\n",
      "(42, 15000)\n",
      "450\n",
      "(42, 15000)\n",
      "451\n",
      "(42, 15000)\n",
      "452\n",
      "(42, 15000)\n",
      "453\n",
      "(42, 15000)\n",
      "454\n",
      "(42, 15000)\n",
      "455\n",
      "(42, 15000)\n",
      "456\n",
      "(42, 15000)\n",
      "457\n",
      "(42, 15000)\n",
      "458\n",
      "(42, 15000)\n",
      "459\n",
      "(42, 15000)\n",
      "460\n",
      "(42, 15000)\n",
      "461\n",
      "(42, 15000)\n",
      "462\n",
      "(42, 15000)\n",
      "463\n",
      "(42, 15000)\n",
      "464\n",
      "(42, 15000)\n",
      "465\n",
      "(42, 15000)\n",
      "466\n",
      "(42, 15000)\n",
      "467\n",
      "(42, 15000)\n",
      "468\n",
      "(42, 15000)\n",
      "469\n",
      "(42, 15000)\n",
      "470\n",
      "(42, 15000)\n",
      "471\n",
      "(42, 15000)\n",
      "472\n",
      "(42, 15000)\n",
      "473\n",
      "(42, 15000)\n",
      "474\n",
      "(42, 15000)\n",
      "475\n",
      "(42, 15000)\n",
      "476\n",
      "(42, 15000)\n",
      "477\n",
      "(42, 15000)\n",
      "478\n",
      "(42, 15000)\n",
      "479\n",
      "(42, 15000)\n",
      "480\n",
      "(42, 15000)\n",
      "481\n",
      "(42, 15000)\n",
      "482\n",
      "(42, 15000)\n",
      "483\n",
      "(42, 15000)\n",
      "484\n",
      "(42, 15000)\n",
      "485\n",
      "(42, 15000)\n",
      "486\n",
      "(42, 15000)\n",
      "487\n",
      "(42, 15000)\n",
      "488\n",
      "(42, 15000)\n",
      "489\n",
      "(42, 15000)\n",
      "490\n",
      "(42, 15000)\n",
      "491\n",
      "(42, 15000)\n",
      "492\n",
      "(42, 15000)\n",
      "493\n",
      "(42, 15000)\n",
      "494\n",
      "(42, 15000)\n",
      "495\n",
      "(42, 15000)\n",
      "496\n",
      "(42, 15000)\n",
      "497\n",
      "(42, 15000)\n",
      "498\n",
      "(42, 15000)\n",
      "499\n",
      "(42, 15000)\n",
      "500\n",
      "(42, 15000)\n",
      "501\n",
      "(42, 15000)\n",
      "502\n",
      "(42, 15000)\n",
      "503\n",
      "(42, 15000)\n",
      "504\n",
      "(42, 15000)\n",
      "505\n",
      "(42, 15000)\n",
      "506\n",
      "(42, 15000)\n",
      "507\n",
      "(42, 15000)\n",
      "508\n",
      "(42, 15000)\n",
      "509\n",
      "(42, 15000)\n",
      "510\n",
      "(42, 15000)\n",
      "511\n",
      "(42, 15000)\n",
      "512\n",
      "(42, 15000)\n",
      "513\n",
      "(42, 15000)\n",
      "514\n",
      "(42, 15000)\n",
      "515\n",
      "(42, 15000)\n",
      "516\n",
      "(42, 15000)\n",
      "517\n",
      "(42, 15000)\n",
      "518\n",
      "(42, 15000)\n",
      "519\n",
      "(42, 15000)\n",
      "520\n",
      "(42, 15000)\n",
      "521\n",
      "(42, 15000)\n",
      "522\n",
      "(42, 15000)\n",
      "523\n",
      "(42, 15000)\n",
      "524\n",
      "(42, 15000)\n",
      "525\n",
      "(42, 15000)\n",
      "526\n",
      "(42, 15000)\n",
      "527\n",
      "(42, 15000)\n",
      "528\n",
      "(42, 15000)\n",
      "529\n",
      "(42, 15000)\n",
      "530\n",
      "(42, 15000)\n",
      "531\n",
      "(42, 15000)\n",
      "532\n",
      "(42, 15000)\n",
      "533\n",
      "(42, 15000)\n",
      "534\n",
      "(42, 15000)\n",
      "535\n",
      "(42, 15000)\n",
      "536\n",
      "(42, 15000)\n",
      "537\n",
      "(42, 15000)\n",
      "538\n",
      "(42, 15000)\n",
      "539\n",
      "(42, 15000)\n",
      "540\n",
      "(42, 15000)\n",
      "541\n",
      "(42, 15000)\n",
      "542\n",
      "(42, 15000)\n",
      "543\n",
      "(42, 15000)\n",
      "544\n",
      "(42, 15000)\n",
      "545\n",
      "(42, 15000)\n",
      "546\n",
      "(42, 15000)\n",
      "547\n",
      "(42, 15000)\n",
      "548\n",
      "(42, 15000)\n",
      "549\n",
      "(42, 15000)\n",
      "550\n",
      "(42, 15000)\n",
      "551\n",
      "(42, 15000)\n",
      "552\n",
      "(42, 15000)\n",
      "553\n",
      "(42, 15000)\n",
      "554\n",
      "(42, 15000)\n",
      "555\n",
      "(42, 15000)\n",
      "556\n",
      "(42, 15000)\n",
      "557\n",
      "(42, 15000)\n",
      "558\n",
      "(42, 15000)\n",
      "559\n",
      "(42, 15000)\n",
      "560\n",
      "(42, 15000)\n",
      "561\n",
      "(42, 15000)\n",
      "562\n",
      "(42, 15000)\n",
      "563\n",
      "(42, 15000)\n",
      "564\n",
      "(42, 15000)\n",
      "565\n",
      "(42, 15000)\n",
      "566\n",
      "(42, 15000)\n",
      "567\n",
      "(42, 15000)\n",
      "568\n",
      "(42, 15000)\n",
      "569\n",
      "(42, 15000)\n",
      "570\n",
      "(42, 15000)\n",
      "571\n",
      "(42, 15000)\n",
      "572\n",
      "(42, 15000)\n",
      "573\n",
      "(42, 15000)\n",
      "574\n",
      "(42, 15000)\n",
      "575\n",
      "(42, 15000)\n",
      "576\n",
      "(42, 15000)\n",
      "577\n",
      "(42, 15000)\n",
      "578\n",
      "(42, 15000)\n",
      "579\n",
      "(42, 15000)\n",
      "580\n",
      "(42, 15000)\n",
      "581\n",
      "(42, 15000)\n",
      "582\n",
      "(42, 15000)\n",
      "583\n",
      "(42, 15000)\n",
      "584\n",
      "(42, 15000)\n",
      "585\n",
      "(42, 15000)\n",
      "586\n",
      "(42, 15000)\n",
      "587\n",
      "(42, 15000)\n",
      "588\n",
      "(42, 15000)\n",
      "589\n",
      "(42, 15000)\n",
      "590\n",
      "(42, 15000)\n",
      "591\n",
      "(42, 15000)\n",
      "592\n",
      "(42, 15000)\n",
      "593\n",
      "(42, 15000)\n",
      "594\n",
      "(42, 15000)\n",
      "595\n",
      "(42, 15000)\n",
      "596\n",
      "(42, 15000)\n",
      "597\n",
      "(42, 15000)\n",
      "598\n",
      "(42, 15000)\n",
      "599\n",
      "(42, 15000)\n",
      "600\n",
      "(42, 15000)\n",
      "601\n",
      "(42, 15000)\n",
      "602\n",
      "(42, 15000)\n",
      "603\n",
      "(42, 15000)\n",
      "604\n",
      "(42, 15000)\n",
      "605\n",
      "(42, 15000)\n",
      "606\n",
      "(42, 15000)\n",
      "607\n",
      "(42, 15000)\n",
      "608\n",
      "(42, 15000)\n",
      "609\n",
      "(42, 15000)\n",
      "610\n",
      "(42, 15000)\n",
      "611\n",
      "(42, 15000)\n",
      "612\n",
      "(42, 15000)\n",
      "613\n",
      "(42, 15000)\n",
      "614\n",
      "(42, 15000)\n",
      "615\n",
      "(42, 15000)\n",
      "616\n",
      "(42, 15000)\n",
      "617\n",
      "(42, 15000)\n",
      "618\n",
      "(42, 15000)\n",
      "619\n",
      "(42, 15000)\n",
      "620\n",
      "(42, 15000)\n",
      "621\n",
      "(42, 15000)\n",
      "622\n",
      "(42, 15000)\n",
      "623\n",
      "(42, 15000)\n",
      "624\n",
      "(42, 15000)\n",
      "625\n",
      "(42, 15000)\n",
      "626\n",
      "(42, 15000)\n",
      "627\n",
      "(42, 15000)\n",
      "628\n",
      "(42, 15000)\n",
      "629\n",
      "(42, 15000)\n",
      "630\n",
      "(42, 15000)\n",
      "631\n",
      "(42, 15000)\n",
      "632\n",
      "(42, 15000)\n",
      "633\n",
      "(42, 15000)\n",
      "634\n",
      "(42, 15000)\n",
      "635\n",
      "(42, 15000)\n",
      "636\n",
      "(42, 15000)\n",
      "637\n",
      "(42, 15000)\n",
      "638\n",
      "(42, 15000)\n",
      "639\n",
      "(42, 15000)\n",
      "640\n",
      "(42, 15000)\n",
      "641\n",
      "(42, 15000)\n",
      "642\n",
      "(42, 15000)\n",
      "643\n",
      "(42, 15000)\n",
      "644\n",
      "(42, 15000)\n",
      "645\n",
      "(42, 15000)\n",
      "646\n",
      "(42, 15000)\n",
      "647\n",
      "(42, 15000)\n",
      "648\n",
      "(42, 15000)\n",
      "649\n",
      "(42, 15000)\n",
      "650\n",
      "(42, 15000)\n",
      "651\n",
      "(42, 15000)\n",
      "652\n",
      "(42, 15000)\n",
      "653\n",
      "(42, 15000)\n",
      "654\n",
      "(42, 15000)\n",
      "655\n",
      "(42, 15000)\n",
      "656\n",
      "(42, 15000)\n",
      "657\n",
      "(42, 15000)\n",
      "658\n",
      "(42, 15000)\n",
      "659\n",
      "(42, 15000)\n",
      "660\n",
      "(42, 15000)\n",
      "661\n",
      "(42, 15000)\n",
      "662\n",
      "(42, 15000)\n",
      "663\n",
      "(42, 15000)\n",
      "664\n",
      "(42, 15000)\n",
      "665\n",
      "(42, 15000)\n",
      "666\n",
      "(42, 15000)\n",
      "667\n",
      "(42, 15000)\n",
      "668\n",
      "(42, 15000)\n",
      "669\n",
      "(42, 15000)\n",
      "670\n",
      "(42, 15000)\n",
      "671\n",
      "(42, 15000)\n",
      "672\n",
      "(42, 15000)\n",
      "673\n",
      "(42, 15000)\n",
      "674\n",
      "(42, 15000)\n",
      "675\n",
      "(42, 15000)\n",
      "676\n",
      "(42, 15000)\n",
      "677\n",
      "(42, 15000)\n",
      "678\n",
      "(42, 15000)\n",
      "679\n",
      "(42, 15000)\n",
      "680\n",
      "(680, 15000, 42)\n",
      "680\n"
     ]
    }
   ],
   "source": [
    "#  处理数据集\n",
    "import segyio\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 定义文件夹路径和文件名格式\n",
    "folder_path = \"../testData/\"\n",
    "file_pattern = \"*.sgy\"\n",
    "\n",
    "# 获取所有匹配的文件名\n",
    "file_list = glob.glob(os.path.join(folder_path, file_pattern))\n",
    "\n",
    "# 定义张量的维度\n",
    "num_files = len(file_list)\n",
    "num_samples = 15000\n",
    "num_channels = 42\n",
    "num_components = 1 # 新增维度 表示数据的分量 \n",
    "\n",
    "# 创建一个空张量来保存所有数据\n",
    "data = np.zeros((num_files, num_samples,num_channels))\n",
    "\n",
    "count = 0\n",
    "# 遍历所有文件并将它们保存到张量中\n",
    "for i, file_name in enumerate(file_list):\n",
    "    with segyio.open(file_name, \"r\", ignore_geometry=True) as segyfile:\n",
    "        trace_data = segyfile.trace.raw[:]\n",
    "        print(trace_data.shape)\n",
    "\n",
    "        #  保存每一个台站的张量  42 x 15000的形状\n",
    "        data[i,:,:] = np.reshape(trace_data, (num_samples,num_channels))\n",
    "\n",
    "        #  扩展第二个维度为1\n",
    "        # data[i] = np.expand_dims(data[i],axis=0)\n",
    "        # data[i,:,:,:] = data[i,:,:].reshape()\n",
    "        # print(\"执行力\")\n",
    "        count += 1\n",
    "        print(count)\n",
    "\n",
    "# 打印张量的形状\n",
    "print(data.shape)\n",
    "print(count)\n",
    "\n",
    "data = data.reshape(680,42,15000,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(680, 42, 15000, 1)\n"
     ]
    }
   ],
   "source": [
    "#  处理数据集\n",
    "# data = data.reshape()\n",
    "print(data.shape)\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练及大小：torch.Size([544, 42, 15000, 1])\n",
      "测试集大小:torch.Size([136, 42, 15000, 1])\n"
     ]
    }
   ],
   "source": [
    "#  使用dataLoader进行加载\n",
    "train_data ,test_data = train_test_split(data,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "#  分割成训练集和测试集\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "print(\"训练及大小：{}\".format(train_data.shape))\n",
    "print(\"测试集大小:{}\".format(test_data.shape))\n",
    "\n",
    "\n",
    "# 使用dataloader加载数据集\n",
    "\n",
    "train_data_loader = DataLoader(train_data,batch_size=16)\n",
    "test_data_loader = DataLoader(test_data,batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地震道数量:42\n",
      "采样点数量：15000\n",
      "训练集大小: torch.Size([33, 15000])\n",
      "测试集大小: torch.Size([9, 15000])\n",
      "torch.Size([33, 1, 15000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 处理数据集\n",
    "# 如果数据是 594 x 1500 将数据处理成 594  x 1 x 1500 按照batch_size 分批\n",
    "\n",
    "import segyio\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 读取SEGY文件\n",
    "segy_file = '../SegyData/20220101_115300.sgy'\n",
    "\n",
    "with segyio.open(segy_file, 'rb') as segy:\n",
    "    # 获取地震数据的一维数组\n",
    "    seismic_data = segy.trace.raw[:]\n",
    "\n",
    "    # 获取地震道数量和每个地震道的时间采样点数量\n",
    "    n_traces = segy.tracecount\n",
    "    n_samples = segy.samples.size\n",
    "\n",
    "    print(\"地震道数量:{}\".format(n_traces))\n",
    "    print(\"采样点数量：{}\".format(n_samples))\n",
    "\n",
    "    # 将一维数组重新形状为二维数组，形状为 (n_traces, n_samples)\n",
    "    seismic_data = seismic_data.reshape((n_traces, n_samples))\n",
    "\n",
    "# 随机划分为训练集和测试集\n",
    "train_data_s, test_data_s = train_test_split(seismic_data, test_size=0.2, random_state=42)\n",
    "#  分割成 训练集和测试集\n",
    "# 将数据转换为PyTorch张量  \n",
    "train_data_s = torch.from_numpy(train_data_s).float()\n",
    "test_data_s = torch.from_numpy(test_data_s).float()\n",
    "\n",
    "print(f\"训练集大小: {train_data_s.shape}\")\n",
    "print(f\"测试集大小: {test_data_s.shape}\")\n",
    "\n",
    "# reshape 42 x 15000 x 1\n",
    "train_data_s = train_data_s.reshape(33,1,15000)\n",
    "test_data_s = test_data_s.reshape(9,1,15000)\n",
    "print(train_data_s.shape)\n",
    "\n",
    "#  数据预处理  归一化 0，1 之间\n",
    "\n",
    "# 上面的数据是 42 x 15000 然后reshape 42 x 1 x 15000\n",
    "\n",
    "#  dataloader加载数据集\n",
    "train_dataloader_s = DataLoader(train_data_s,batch_size = 3)\n",
    "test_dataloader_s = DataLoader(test_data_s,batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 定义自编码器网络\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "\n",
    "        #  编码器 最后输出 1000 x 1 的结果\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1) ,# 1D convolutional layer with 16 filters\n",
    "\n",
    "            #  15000 x 16 -> 5000 x 16\n",
    "            nn.MaxPool1d(3) ,# Max pooling layer with compression_ratio stride\n",
    "       \n",
    "            # 5000 x 16  -> 5000 x 32\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1) ,# 1D convolutional layer with 64 filters\n",
    "\n",
    "            #  5000 x 32 -> 5000 x 64\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1) ,# 1D convolutional layer with 64 filters\n",
    "\n",
    "            # 5000 x 64-> 1000 x 64\n",
    "            nn.MaxPool1d(5),\n",
    "\n",
    "            # 1000 x 64 -> 1000 x 128\n",
    "            nn.Conv1d(64,128,kernel_size=3,padding=1),\n",
    "\n",
    "            # 最后一层卷积 1000 x 128 -> 1000 x 1\n",
    "            nn.Conv1d(128,1,kernel_size=3,padding=1)\n",
    "        )\n",
    "\n",
    "        # 解码器网络\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 100 x 1 -> 100 x 128\n",
    "            nn.ConvTranspose1d(1,128,kernel_size=3,padding=1),\n",
    "\n",
    "            # 100 x 128 -> 500 x 128\n",
    "            nn.Upsample(scale_factor=5),\n",
    "\n",
    "            #  500 x 128 -> 500 x 64\n",
    "            nn.ConvTranspose1d(128,64,kernel_size=3,padding=1),\n",
    "\n",
    "            # 500 x 64 -> 500 x 32\n",
    "            nn.ConvTranspose1d(64,32,kernel_size=3,padding=1),\n",
    "            \n",
    "            # 500 x 32 -> 1500 x 32\n",
    "            nn.Upsample(scale_factor=3),\n",
    "\n",
    "            #  1500 x 32 -> 1500 x 16\n",
    "            nn.ConvTranspose1d(32,16,kernel_size=3,padding=1),\n",
    "\n",
    "            #  1500 x 16 -> 1500 x 1\n",
    "            nn.ConvTranspose1d(16,1,kernel_size=3,padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------第1轮训练开始\n",
      "训练次数:2210,Loss0.2720562815666199\n",
      "-------第2轮训练开始\n",
      "训练次数:2220,Loss0.2605443000793457\n",
      "-------第3轮训练开始\n",
      "训练次数:2230,Loss0.26640787720680237\n",
      "-------第4轮训练开始\n",
      "训练次数:2240,Loss0.2464098483324051\n",
      "-------第5轮训练开始\n",
      "训练次数:2250,Loss0.26119691133499146\n",
      "-------第6轮训练开始\n",
      "训练次数:2260,Loss0.27709704637527466\n",
      "-------第7轮训练开始\n",
      "训练次数:2270,Loss0.27440446615219116\n",
      "-------第8轮训练开始\n",
      "训练次数:2280,Loss0.2797960042953491\n",
      "-------第9轮训练开始\n",
      "训练次数:2290,Loss0.27016228437423706\n",
      "-------第10轮训练开始\n",
      "训练次数:2300,Loss0.2701079547405243\n",
      "训练次数:2310,Loss0.265067458152771\n",
      "-------第11轮训练开始\n",
      "训练次数:2320,Loss0.2720562815666199\n",
      "-------第12轮训练开始\n",
      "训练次数:2330,Loss0.2605443000793457\n",
      "-------第13轮训练开始\n",
      "训练次数:2340,Loss0.26640787720680237\n",
      "-------第14轮训练开始\n",
      "训练次数:2350,Loss0.2464098483324051\n",
      "-------第15轮训练开始\n",
      "训练次数:2360,Loss0.26119691133499146\n",
      "-------第16轮训练开始\n",
      "训练次数:2370,Loss0.27709704637527466\n",
      "-------第17轮训练开始\n",
      "训练次数:2380,Loss0.27440446615219116\n",
      "-------第18轮训练开始\n",
      "训练次数:2390,Loss0.2797960042953491\n",
      "-------第19轮训练开始\n",
      "训练次数:2400,Loss0.27016228437423706\n",
      "-------第20轮训练开始\n",
      "训练次数:2410,Loss0.2701079547405243\n",
      "训练次数:2420,Loss0.265067458152771\n",
      "-------第21轮训练开始\n",
      "训练次数:2430,Loss0.2720562815666199\n",
      "-------第22轮训练开始\n",
      "训练次数:2440,Loss0.2605443000793457\n",
      "-------第23轮训练开始\n",
      "训练次数:2450,Loss0.26640787720680237\n",
      "-------第24轮训练开始\n",
      "训练次数:2460,Loss0.2464098483324051\n",
      "-------第25轮训练开始\n",
      "训练次数:2470,Loss0.26119691133499146\n",
      "-------第26轮训练开始\n",
      "训练次数:2480,Loss0.27709704637527466\n",
      "-------第27轮训练开始\n",
      "训练次数:2490,Loss0.27440446615219116\n",
      "-------第28轮训练开始\n",
      "训练次数:2500,Loss0.2797960042953491\n",
      "-------第29轮训练开始\n",
      "训练次数:2510,Loss0.27016228437423706\n",
      "-------第30轮训练开始\n",
      "训练次数:2520,Loss0.2701079547405243\n",
      "训练次数:2530,Loss0.265067458152771\n",
      "-------第31轮训练开始\n",
      "训练次数:2540,Loss0.2720562815666199\n",
      "-------第32轮训练开始\n",
      "训练次数:2550,Loss0.2605443000793457\n",
      "-------第33轮训练开始\n",
      "训练次数:2560,Loss0.26640787720680237\n",
      "-------第34轮训练开始\n",
      "训练次数:2570,Loss0.2464098483324051\n",
      "-------第35轮训练开始\n",
      "训练次数:2580,Loss0.26119691133499146\n",
      "-------第36轮训练开始\n",
      "训练次数:2590,Loss0.27709704637527466\n",
      "-------第37轮训练开始\n",
      "训练次数:2600,Loss0.27440446615219116\n",
      "-------第38轮训练开始\n",
      "训练次数:2610,Loss0.2797960042953491\n",
      "-------第39轮训练开始\n",
      "训练次数:2620,Loss0.27016228437423706\n",
      "-------第40轮训练开始\n",
      "训练次数:2630,Loss0.2701079547405243\n",
      "训练次数:2640,Loss0.265067458152771\n",
      "-------第41轮训练开始\n",
      "训练次数:2650,Loss0.2720562815666199\n",
      "-------第42轮训练开始\n",
      "训练次数:2660,Loss0.2605443000793457\n",
      "-------第43轮训练开始\n",
      "训练次数:2670,Loss0.26640787720680237\n",
      "-------第44轮训练开始\n",
      "训练次数:2680,Loss0.2464098483324051\n",
      "-------第45轮训练开始\n",
      "训练次数:2690,Loss0.26119691133499146\n",
      "-------第46轮训练开始\n",
      "训练次数:2700,Loss0.27709704637527466\n",
      "-------第47轮训练开始\n",
      "训练次数:2710,Loss0.27440446615219116\n",
      "-------第48轮训练开始\n",
      "训练次数:2720,Loss0.2797960042953491\n",
      "-------第49轮训练开始\n",
      "训练次数:2730,Loss0.27016228437423706\n",
      "-------第50轮训练开始\n",
      "训练次数:2740,Loss0.2701079547405243\n",
      "训练次数:2750,Loss0.265067458152771\n",
      "-------第51轮训练开始\n",
      "训练次数:2760,Loss0.2720562815666199\n",
      "-------第52轮训练开始\n",
      "训练次数:2770,Loss0.2605443000793457\n",
      "-------第53轮训练开始\n",
      "训练次数:2780,Loss0.26640787720680237\n",
      "-------第54轮训练开始\n",
      "训练次数:2790,Loss0.2464098483324051\n",
      "-------第55轮训练开始\n",
      "训练次数:2800,Loss0.26119691133499146\n",
      "-------第56轮训练开始\n",
      "训练次数:2810,Loss0.27709704637527466\n",
      "-------第57轮训练开始\n",
      "训练次数:2820,Loss0.27440446615219116\n",
      "-------第58轮训练开始\n",
      "训练次数:2830,Loss0.2797960042953491\n",
      "-------第59轮训练开始\n",
      "训练次数:2840,Loss0.27016228437423706\n",
      "-------第60轮训练开始\n",
      "训练次数:2850,Loss0.2701079547405243\n",
      "训练次数:2860,Loss0.265067458152771\n",
      "-------第61轮训练开始\n",
      "训练次数:2870,Loss0.2720562815666199\n",
      "-------第62轮训练开始\n",
      "训练次数:2880,Loss0.2605443000793457\n",
      "-------第63轮训练开始\n",
      "训练次数:2890,Loss0.26640787720680237\n",
      "-------第64轮训练开始\n",
      "训练次数:2900,Loss0.2464098483324051\n",
      "-------第65轮训练开始\n",
      "训练次数:2910,Loss0.26119691133499146\n",
      "-------第66轮训练开始\n",
      "训练次数:2920,Loss0.27709704637527466\n",
      "-------第67轮训练开始\n",
      "训练次数:2930,Loss0.27440446615219116\n",
      "-------第68轮训练开始\n",
      "训练次数:2940,Loss0.2797960042953491\n",
      "-------第69轮训练开始\n",
      "训练次数:2950,Loss0.27016228437423706\n",
      "-------第70轮训练开始\n",
      "训练次数:2960,Loss0.2701079547405243\n",
      "训练次数:2970,Loss0.265067458152771\n",
      "-------第71轮训练开始\n",
      "训练次数:2980,Loss0.2720562815666199\n",
      "-------第72轮训练开始\n",
      "训练次数:2990,Loss0.2605443000793457\n",
      "-------第73轮训练开始\n",
      "训练次数:3000,Loss0.26640787720680237\n",
      "-------第74轮训练开始\n",
      "训练次数:3010,Loss0.2464098483324051\n",
      "-------第75轮训练开始\n",
      "训练次数:3020,Loss0.26119691133499146\n",
      "-------第76轮训练开始\n",
      "训练次数:3030,Loss0.27709704637527466\n",
      "-------第77轮训练开始\n",
      "训练次数:3040,Loss0.27440446615219116\n",
      "-------第78轮训练开始\n",
      "训练次数:3050,Loss0.2797960042953491\n",
      "-------第79轮训练开始\n",
      "训练次数:3060,Loss0.27016228437423706\n",
      "-------第80轮训练开始\n",
      "训练次数:3070,Loss0.2701079547405243\n",
      "训练次数:3080,Loss0.265067458152771\n",
      "-------第81轮训练开始\n",
      "训练次数:3090,Loss0.2720562815666199\n",
      "-------第82轮训练开始\n",
      "训练次数:3100,Loss0.2605443000793457\n",
      "-------第83轮训练开始\n",
      "训练次数:3110,Loss0.26640787720680237\n",
      "-------第84轮训练开始\n",
      "训练次数:3120,Loss0.2464098483324051\n",
      "-------第85轮训练开始\n",
      "训练次数:3130,Loss0.26119691133499146\n",
      "-------第86轮训练开始\n",
      "训练次数:3140,Loss0.27709704637527466\n",
      "-------第87轮训练开始\n",
      "训练次数:3150,Loss0.27440446615219116\n",
      "-------第88轮训练开始\n",
      "训练次数:3160,Loss0.2797960042953491\n",
      "-------第89轮训练开始\n",
      "训练次数:3170,Loss0.27016228437423706\n",
      "-------第90轮训练开始\n",
      "训练次数:3180,Loss0.2701079547405243\n",
      "训练次数:3190,Loss0.265067458152771\n",
      "-------第91轮训练开始\n",
      "训练次数:3200,Loss0.2720562815666199\n",
      "-------第92轮训练开始\n",
      "训练次数:3210,Loss0.2605443000793457\n",
      "-------第93轮训练开始\n",
      "训练次数:3220,Loss0.26640787720680237\n",
      "-------第94轮训练开始\n",
      "训练次数:3230,Loss0.2464098483324051\n",
      "-------第95轮训练开始\n",
      "训练次数:3240,Loss0.26119691133499146\n",
      "-------第96轮训练开始\n",
      "训练次数:3250,Loss0.27709704637527466\n",
      "-------第97轮训练开始\n",
      "训练次数:3260,Loss0.27440446615219116\n",
      "-------第98轮训练开始\n",
      "训练次数:3270,Loss0.2797960042953491\n",
      "-------第99轮训练开始\n",
      "训练次数:3280,Loss0.27016228437423706\n",
      "-------第100轮训练开始\n",
      "训练次数:3290,Loss0.2701079547405243\n",
      "训练次数:3300,Loss0.265067458152771\n",
      "-------第101轮训练开始\n",
      "训练次数:3310,Loss0.2720562815666199\n",
      "-------第102轮训练开始\n",
      "训练次数:3320,Loss0.2605443000793457\n",
      "-------第103轮训练开始\n",
      "训练次数:3330,Loss0.26640787720680237\n",
      "-------第104轮训练开始\n",
      "训练次数:3340,Loss0.2464098483324051\n",
      "-------第105轮训练开始\n",
      "训练次数:3350,Loss0.26119691133499146\n",
      "-------第106轮训练开始\n",
      "训练次数:3360,Loss0.27709704637527466\n",
      "-------第107轮训练开始\n",
      "训练次数:3370,Loss0.27440446615219116\n",
      "-------第108轮训练开始\n",
      "训练次数:3380,Loss0.2797960042953491\n",
      "-------第109轮训练开始\n",
      "训练次数:3390,Loss0.27016228437423706\n",
      "-------第110轮训练开始\n",
      "训练次数:3400,Loss0.2701079547405243\n",
      "训练次数:3410,Loss0.265067458152771\n",
      "-------第111轮训练开始\n",
      "训练次数:3420,Loss0.2720562815666199\n",
      "-------第112轮训练开始\n",
      "训练次数:3430,Loss0.2605443000793457\n",
      "-------第113轮训练开始\n",
      "训练次数:3440,Loss0.26640787720680237\n",
      "-------第114轮训练开始\n",
      "训练次数:3450,Loss0.2464098483324051\n",
      "-------第115轮训练开始\n",
      "训练次数:3460,Loss0.26119691133499146\n",
      "-------第116轮训练开始\n",
      "训练次数:3470,Loss0.27709704637527466\n",
      "-------第117轮训练开始\n",
      "训练次数:3480,Loss0.27440446615219116\n",
      "-------第118轮训练开始\n",
      "训练次数:3490,Loss0.2797960042953491\n",
      "-------第119轮训练开始\n",
      "训练次数:3500,Loss0.27016228437423706\n",
      "-------第120轮训练开始\n",
      "训练次数:3510,Loss0.2701079547405243\n",
      "训练次数:3520,Loss0.265067458152771\n",
      "-------第121轮训练开始\n",
      "训练次数:3530,Loss0.2720562815666199\n",
      "-------第122轮训练开始\n",
      "训练次数:3540,Loss0.2605443000793457\n",
      "-------第123轮训练开始\n",
      "训练次数:3550,Loss0.26640787720680237\n",
      "-------第124轮训练开始\n",
      "训练次数:3560,Loss0.2464098483324051\n",
      "-------第125轮训练开始\n",
      "训练次数:3570,Loss0.26119691133499146\n",
      "-------第126轮训练开始\n",
      "训练次数:3580,Loss0.27709704637527466\n",
      "-------第127轮训练开始\n",
      "训练次数:3590,Loss0.27440446615219116\n",
      "-------第128轮训练开始\n",
      "训练次数:3600,Loss0.2797960042953491\n",
      "-------第129轮训练开始\n",
      "训练次数:3610,Loss0.27016228437423706\n",
      "-------第130轮训练开始\n",
      "训练次数:3620,Loss0.2701079547405243\n",
      "训练次数:3630,Loss0.265067458152771\n",
      "-------第131轮训练开始\n",
      "训练次数:3640,Loss0.2720562815666199\n",
      "-------第132轮训练开始\n",
      "训练次数:3650,Loss0.2605443000793457\n",
      "-------第133轮训练开始\n",
      "训练次数:3660,Loss0.26640787720680237\n",
      "-------第134轮训练开始\n",
      "训练次数:3670,Loss0.2464098483324051\n",
      "-------第135轮训练开始\n",
      "训练次数:3680,Loss0.26119691133499146\n",
      "-------第136轮训练开始\n",
      "训练次数:3690,Loss0.27709704637527466\n",
      "-------第137轮训练开始\n",
      "训练次数:3700,Loss0.27440446615219116\n",
      "-------第138轮训练开始\n",
      "训练次数:3710,Loss0.2797960042953491\n",
      "-------第139轮训练开始\n",
      "训练次数:3720,Loss0.27016228437423706\n",
      "-------第140轮训练开始\n",
      "训练次数:3730,Loss0.2701079547405243\n",
      "训练次数:3740,Loss0.265067458152771\n",
      "-------第141轮训练开始\n",
      "训练次数:3750,Loss0.2720562815666199\n",
      "-------第142轮训练开始\n",
      "训练次数:3760,Loss0.2605443000793457\n",
      "-------第143轮训练开始\n",
      "训练次数:3770,Loss0.26640787720680237\n",
      "-------第144轮训练开始\n",
      "训练次数:3780,Loss0.2464098483324051\n",
      "-------第145轮训练开始\n",
      "训练次数:3790,Loss0.26119691133499146\n",
      "-------第146轮训练开始\n",
      "训练次数:3800,Loss0.27709704637527466\n",
      "-------第147轮训练开始\n",
      "训练次数:3810,Loss0.27440446615219116\n",
      "-------第148轮训练开始\n",
      "训练次数:3820,Loss0.2797960042953491\n",
      "-------第149轮训练开始\n",
      "训练次数:3830,Loss0.27016228437423706\n",
      "-------第150轮训练开始\n",
      "训练次数:3840,Loss0.2701079547405243\n",
      "训练次数:3850,Loss0.265067458152771\n",
      "-------第151轮训练开始\n",
      "训练次数:3860,Loss0.2720562815666199\n",
      "-------第152轮训练开始\n",
      "训练次数:3870,Loss0.2605443000793457\n",
      "-------第153轮训练开始\n",
      "训练次数:3880,Loss0.26640787720680237\n",
      "-------第154轮训练开始\n",
      "训练次数:3890,Loss0.2464098483324051\n",
      "-------第155轮训练开始\n",
      "训练次数:3900,Loss0.26119691133499146\n",
      "-------第156轮训练开始\n",
      "训练次数:3910,Loss0.27709704637527466\n",
      "-------第157轮训练开始\n",
      "训练次数:3920,Loss0.27440446615219116\n",
      "-------第158轮训练开始\n",
      "训练次数:3930,Loss0.2797960042953491\n",
      "-------第159轮训练开始\n",
      "训练次数:3940,Loss0.27016228437423706\n",
      "-------第160轮训练开始\n",
      "训练次数:3950,Loss0.2701079547405243\n",
      "训练次数:3960,Loss0.265067458152771\n",
      "-------第161轮训练开始\n",
      "训练次数:3970,Loss0.2720562815666199\n",
      "-------第162轮训练开始\n",
      "训练次数:3980,Loss0.2605443000793457\n",
      "-------第163轮训练开始\n",
      "训练次数:3990,Loss0.26640787720680237\n",
      "-------第164轮训练开始\n",
      "训练次数:4000,Loss0.2464098483324051\n",
      "-------第165轮训练开始\n",
      "训练次数:4010,Loss0.26119691133499146\n",
      "-------第166轮训练开始\n",
      "训练次数:4020,Loss0.27709704637527466\n",
      "-------第167轮训练开始\n",
      "训练次数:4030,Loss0.27440446615219116\n",
      "-------第168轮训练开始\n",
      "训练次数:4040,Loss0.2797960042953491\n",
      "-------第169轮训练开始\n",
      "训练次数:4050,Loss0.27016228437423706\n",
      "-------第170轮训练开始\n",
      "训练次数:4060,Loss0.2701079547405243\n",
      "训练次数:4070,Loss0.265067458152771\n",
      "-------第171轮训练开始\n",
      "训练次数:4080,Loss0.2720562815666199\n",
      "-------第172轮训练开始\n",
      "训练次数:4090,Loss0.2605443000793457\n",
      "-------第173轮训练开始\n",
      "训练次数:4100,Loss0.26640787720680237\n",
      "-------第174轮训练开始\n",
      "训练次数:4110,Loss0.2464098483324051\n",
      "-------第175轮训练开始\n",
      "训练次数:4120,Loss0.26119691133499146\n",
      "-------第176轮训练开始\n",
      "训练次数:4130,Loss0.27709704637527466\n",
      "-------第177轮训练开始\n",
      "训练次数:4140,Loss0.27440446615219116\n",
      "-------第178轮训练开始\n",
      "训练次数:4150,Loss0.2797960042953491\n",
      "-------第179轮训练开始\n",
      "训练次数:4160,Loss0.27016228437423706\n",
      "-------第180轮训练开始\n",
      "训练次数:4170,Loss0.2701079547405243\n",
      "训练次数:4180,Loss0.265067458152771\n",
      "-------第181轮训练开始\n",
      "训练次数:4190,Loss0.2720562815666199\n",
      "-------第182轮训练开始\n",
      "训练次数:4200,Loss0.2605443000793457\n",
      "-------第183轮训练开始\n",
      "训练次数:4210,Loss0.26640787720680237\n",
      "-------第184轮训练开始\n",
      "训练次数:4220,Loss0.2464098483324051\n",
      "-------第185轮训练开始\n",
      "训练次数:4230,Loss0.26119691133499146\n",
      "-------第186轮训练开始\n",
      "训练次数:4240,Loss0.27709704637527466\n",
      "-------第187轮训练开始\n",
      "训练次数:4250,Loss0.27440446615219116\n",
      "-------第188轮训练开始\n",
      "训练次数:4260,Loss0.2797960042953491\n",
      "-------第189轮训练开始\n",
      "训练次数:4270,Loss0.27016228437423706\n",
      "-------第190轮训练开始\n",
      "训练次数:4280,Loss0.2701079547405243\n",
      "训练次数:4290,Loss0.265067458152771\n",
      "-------第191轮训练开始\n",
      "训练次数:4300,Loss0.2720562815666199\n",
      "-------第192轮训练开始\n",
      "训练次数:4310,Loss0.2605443000793457\n",
      "-------第193轮训练开始\n",
      "训练次数:4320,Loss0.26640787720680237\n",
      "-------第194轮训练开始\n",
      "训练次数:4330,Loss0.2464098483324051\n",
      "-------第195轮训练开始\n",
      "训练次数:4340,Loss0.26119691133499146\n",
      "-------第196轮训练开始\n",
      "训练次数:4350,Loss0.27709704637527466\n",
      "-------第197轮训练开始\n",
      "训练次数:4360,Loss0.27440446615219116\n",
      "-------第198轮训练开始\n",
      "训练次数:4370,Loss0.2797960042953491\n",
      "-------第199轮训练开始\n",
      "训练次数:4380,Loss0.27016228437423706\n",
      "-------第200轮训练开始\n",
      "训练次数:4390,Loss0.2701079547405243\n",
      "训练次数:4400,Loss0.265067458152771\n"
     ]
    }
   ],
   "source": [
    "autoCoder = AutoEncoder()\n",
    "writer = SummaryWriter(\"../encoder_train\")\n",
    "epoch = 200\n",
    "for i in range(epoch):\n",
    "    print(\"-------第{}轮训练开始\".format(i + 1))\n",
    "\n",
    "    #  训练步骤开始\n",
    "    autoCoder.train()\n",
    "\n",
    "    for data in train_dataloader_s:\n",
    "        inputs = data\n",
    "        #  将梯度清零\n",
    "       \n",
    "        # 前向传播\n",
    "        outputs = autoCoder(inputs)\n",
    "        # 计算损失  对比原始输入和 自编码器输出的结果 看看压缩效果\n",
    "        loss = loss_fn(outputs,inputs)\n",
    "        # 反向传播 计算梯度\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 优化\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计训练次数\n",
    "        total_train_step = total_train_step + 1\n",
    "\n",
    "        if total_train_step % 10 == 0:\n",
    "            # 绘制训练损失\n",
    "            writer.add_scalar(\"train_A_loss\",loss.item(),total_train_step)\n",
    "            print(\"训练次数:{},Loss{}\".format(total_train_step,loss.item()))\n",
    "\n",
    "\n",
    "    # #  测试步骤开始\n",
    "    # autoCoder.eval()\n",
    "    # total_test_loss = 0\n",
    "    # total_accuracy = 0\n",
    "    # with torch.no_grad():\n",
    "\n",
    "    #     #  取出测试数据集的数据\n",
    "    #     for data in test_dataloader_s:\n",
    "\n",
    "    #         inputs = data\n",
    "\n",
    "    #         outputs = autoCoder(inputs)\n",
    "\n",
    "    #         # #  取出数据\n",
    "    #         # imgs = data\n",
    "    #         # # imgs = imgs.to(device)\n",
    "    #         # # targets = targets.to(device)\n",
    "\n",
    "    #         # outputs = tudui(imgs)\n",
    "\n",
    "    #         loss = loss_fn(outputs,inputs) # 计算损失\n",
    "    #         optimizer.zero_grad()\n",
    "\n",
    "    #         loss.backward()\n",
    "\n",
    "    #         optimizer.step()\n",
    "\n",
    "    #         #  统计测试集上面的总损失\n",
    "    #         total_test_loss = total_test_loss + loss.item()\n",
    "    #         accuracy = (outputs.argmax(1) == inputs).sum()\n",
    "    #         total_accuracy = total_accuracy + accuracy\n",
    "\n",
    "\n",
    "    # print(\"整体测试集上面的Loss:{}\".format(total_test_loss))\n",
    "    # print(\"整体测试及上面的正确率:{}\".format(total_accuracy / test_data_size))\n",
    "    # # writer.add_scalar(\"test_loss\",loss.item(),total_test_step)\n",
    "    # # writer.add_scalar(\"test_accuracy\",total_accuracy / test_data_size,total_test_step)\n",
    "    # total_test_step = total_test_step + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
