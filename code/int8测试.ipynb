{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "[tensor([[ 1.0503, -0.4696, -1.0716,  0.1848],\n",
      "        [-0.2722, -1.7757, -0.2725,  1.0091],\n",
      "        [-0.3572,  1.0260, -0.4611, -1.3358]]), tensor([[ 0.5702,  3.0193,  0.7060, -0.6825],\n",
      "        [-0.1247,  0.5108,  0.0955,  0.3479],\n",
      "        [ 1.8009,  1.8067,  1.3069, -0.2442]])]\n",
      "MinMaxObserver (tensor([0.0188]), tensor([94], dtype=torch.int32))\n",
      "MovingAverageMinMaxObserver (tensor([0.0111]), tensor([159], dtype=torch.int32))\n",
      "HistogramObserver (tensor([0.0188]), tensor([94], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
    "\n",
    "# 定义通道数和序列长度\n",
    "C, L = 3, 4\n",
    "# 创建一个正太分布随机数生成器\n",
    "normal = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "# 生成两个随机输入张量\n",
    "inputs = [normal.sample((C, L)), normal.sample((C, L))]\n",
    "\n",
    "for x in inputs:\n",
    "    print(x.shape)\n",
    "\n",
    "print(inputs)\n",
    "# [tensor([[-0.0590,  1.1674,  0.7119, -1.1270],\n",
    "#          [-1.3974,  0.5077, -0.5601,  0.0683],\n",
    "#          [-0.0929,  0.9473,  0.7159, -0.4574]]]),\n",
    "\n",
    "# tensor([[-0.0236, -0.7599,  1.0290,  0.8914],\n",
    "#          [-1.1727, -1.2556, -0.2271,  0.9568],\n",
    "#          [-0.2500,  1.4579,  1.4707,  0.4043]])]\n",
    "\n",
    "# 创建观察者对象\n",
    "observers = [MinMaxObserver(),          # 最小值、最大值 观察者\n",
    "             MovingAverageMinMaxObserver(),     # 移动平均最小值、最大值 观察者\n",
    "             HistogramObserver()]\n",
    "\n",
    "# 遍历观察者对象列表\n",
    "# 对于每一个观察者 遍历输入数据并且使用观察者对象 观察每一个输入张量\n",
    "# 打印观察者的类名以及观察之后的计算的量化参数 这些参数包括量化的最小值和最大值\n",
    "\n",
    "\n",
    "for obs in observers:\n",
    "    for x in inputs:\n",
    "        obs(x) # 使用观察者对象 观察输入数据\n",
    "    print(obs.__class__.__name__, obs.calculate_qparams())\n",
    "    # MinMaxObserver (tensor([0.0112]), tensor([124], dtype=torch.int32))\n",
    "    # MovingAverageMinMaxObserver (tensor([0.0101]), tensor([139], dtype=torch.int32))\n",
    "    # HistogramObserver (tensor([0.0100]), tensor([106], dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qscheme: torch.per_tensor_affine | (tensor([0.0111]), tensor([159], dtype=torch.int32))\n",
      "Qscheme: torch.per_tensor_symmetric | (tensor([0.0138]), tensor([128]))\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    # 计算移动最大平均值 和最小平均值\n",
    "    obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
    "    for x in inputs:\n",
    "        obs(x)\n",
    "    print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")\n",
    "    # Qscheme: torch.per_tensor_affine | (tensor([0.0101]), tensor([139], dtype=torch.int32))\n",
    "    # Qscheme: torch.per_tensor_symmetric | (tensor([0.0109]), tensor([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0084, 0.0108, 0.0092]), tensor([127, 162, 143], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
    "\n",
    "# 通道参数量化\n",
    "obs =MovingAveragePerChannelMinMaxObserver(ch_axis=0)  # 分别计算所有' C '通道的qparams\n",
    "for x in inputs:\n",
    "    obs(x)\n",
    "print(obs.calculate_qparams())\n",
    "# (tensor([0.0090, 0.0075, 0.0055]), tensor([125, 187,  82], dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x86' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\DeepLearning\\code\\int8测试.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DeepLearning/code/int8%E6%B5%8B%E8%AF%95.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m backend \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfbgemm\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m x86 \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mqnnpack\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepLearning/code/int8%E6%B5%8B%E8%AF%95.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m qconfig \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mquantization\u001b[39m.\u001b[39mget_default_qconfig(backend)  \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepLearning/code/int8%E6%B5%8B%E8%AF%95.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mquantized\u001b[39m.\u001b[39mengine \u001b[39m=\u001b[39m backend\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x86' is not defined"
     ]
    }
   ],
   "source": [
    "backend = 'fbgemm' if x86 else 'qnnpack'\n",
    "qconfig = torch.quantization.get_default_qconfig(backend)  \n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_qconfig = torch.quantization.QConfig(\n",
    "  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine),\n",
    "  weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8)\n",
    ")\n",
    "# >>>>>\n",
    "# QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5235,  0.8499],\n",
      "        [-3.1486,  0.0299]])\n",
      "tensor([[-0.5000,  1.0000],\n",
      "        [-3.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 2, dtype=torch.float32)\n",
    "# tensor([[ 0.9872, -1.6833],\n",
    "#         [-0.9345,  0.6531]])\n",
    "\n",
    "print(x)\n",
    "\n",
    "# 公式1(量化)：xq = round(x / scale + zero_point)\n",
    "# 使用给定的scale和 zero_point 来把一个float tensor转化为 quantized tensor\n",
    "xq = torch.quantize_per_tensor(x, scale=0.5, zero_point=8, dtype=torch.quint8)\n",
    "# tensor([[ 1.0000, -1.5000],\n",
    "#         [-1.0000,  0.5000]], size=(2, 2), dtype=torch.quint8,\n",
    "#        quantization_scheme=torch.per_tensor_affine, scale=0.5, zero_point=8)\n",
    "\n",
    "# print(xq.int_repr())  # 给定一个量化的张量，返回一个以 uint8_t 作为数据类型的张量\n",
    "# tensor([[10,  5],\n",
    "#         [ 6,  9]], dtype=torch.uint8)\n",
    "\n",
    "# 公式2(反量化)：xdq = (xq - zero_point) * scale\n",
    "# 使用给定的scale和 zero_point 来把一个 quantized tensor 转化为 float tensor\n",
    "xdq = xq.dequantize()\n",
    "# tensor([[ 1.0000, -1.5000],\n",
    "#         [-1.0000,  0.5000]])\n",
    "\n",
    "print(xdq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F32Model(\n",
      "  (fc): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "F32Model(\n",
      "  (fc): LinearReLU(\n",
      "    (0): Linear(in_features=3, out_features=2, bias=False)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (relu): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class F32Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F32Model,self).__init__()\n",
    "        self.fc = nn.Linear(3,2,bias=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "model_fp32 = F32Model()\n",
    "\n",
    "print(model_fp32)\n",
    "\n",
    "\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32,[['fc','relu']])\n",
    "\n",
    "print(model_fp32_fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#如果要部署在ARM上\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('qnnpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Application\\Anaconda\\envs\\AI\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:310: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    }
   ],
   "source": [
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time (float32): 0.00096154 seconds\n",
      "Execution time (int8): 0.00000000 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class F32Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F32Model,self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()  # 转换张量从浮点到量化\n",
    "        \n",
    "        \n",
    "        self.conv =  nn.Conv2d(1,1,1)\n",
    "        self.fc = nn.Linear(2,2,bias = False)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 将量化张量  转换为浮点\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model_fp32 = F32Model()\n",
    "\n",
    "# 量化需要开启验证模式\n",
    "model_fp32.eval()\n",
    "\n",
    "# 将模型部署在arm\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "#  将网络的一些层进行融合\n",
    "\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32,[['fc','relu']])\n",
    "\n",
    "#  准备模型  插入观察对象    观察activation 和weight\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "\n",
    "# 代表性数据集 获取与数据的分布特点  来更好的计算 及或者的scale 和zp\n",
    "\n",
    "# batch x channel x h x w\n",
    "input_fp32 = torch.randn(1,1,2,2)\n",
    "\n",
    "#  喂数据  计算参数\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "\n",
    "# 量化模型\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# 运行模型  计算都以int8来计算\n",
    "\n",
    "import time\n",
    "\n",
    "# 测量float32模型的执行时间\n",
    "start_time_fp32 = time.time()\n",
    "output_fp32 = model_fp32(input_fp32)\n",
    "# end_time_fp32 = \n",
    "execution_time_fp32 = time.time() - start_time_fp32\n",
    "\n",
    "# 测量int8模型的执行时间\n",
    "start_time_int8 = time.time()\n",
    "output_int8 = model_int8(input_fp32)\n",
    "# end_time_int8 = \n",
    "execution_time_int8 = time.time() - start_time_int8\n",
    "\n",
    "print(\"Execution time (float32): {:.8f} seconds\".format(execution_time_fp32))\n",
    "print(\"Execution time (int8): {:.8f} seconds\".format(execution_time_int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F32Model(\n",
      "  (quant): Quantize(scale=tensor([0.0068]), zero_point=tensor([45]), dtype=torch.quint8)\n",
      "  (conv): QuantizedConv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.006129027809947729, zero_point=0)\n",
      "  (fc): QuantizedLinearReLU(in_features=2, out_features=2, scale=0.004328194074332714, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "  (relu): Identity()\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Application\\Anaconda\\envs\\AI\\lib\\site-packages\\torch\\_utils.py:314: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_int8.state_dict(), \"./state_dict.pth\")\n",
    "model_int8.load_state_dict(torch.load(\"./state_dict.pth\"))\n",
    "print(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[127, 123],\n",
      "        [ 97,   0]], dtype=torch.int8)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_int8.fc.weight().int_repr())\n",
    "print(model_int8.fc.bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "# 部署的后端计算引擎  运行在x86 芯片\n",
    "backend = \"fbgemm\"\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(2,64,3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64,128,3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "m = copy.deepcopy(model)\n",
    "\n",
    "m.eval()\n",
    "\n",
    "#  然后 开始融合模型\n",
    "torch.quantization.fuse_modules(m,['0','1'],inplace=True)\n",
    "torch.quantization.fuse_modules(m,['2','3'],inplace=True)\n",
    "\n",
    "# 插入Stub\n",
    "\n",
    "m = nn.Sequential(\n",
    "    torch.quantization.QuantStub(),\n",
    "    m,\n",
    "    torch.quantization.DeQuantStub()\n",
    ")\n",
    "\n",
    "# 设置后端\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "\n",
    "#  插入观察对象\n",
    "torch.quantization.prepare(m,inplace = True)\n",
    "\n",
    "\n",
    "# 喂数据  计算scale和zero_point\n",
    "\n",
    "#  推理模式  没有反向传播计算\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        x = torch.rand(1,2,28,28)\n",
    "        m(x)\n",
    "    \n",
    "# 转换为int8量化模型 \n",
    "torch.quantization.convert(m,inplace=True)\n",
    "\n",
    "# 检查一下 权重参数是不是Int8 \n",
    "\n",
    "# print(m[[1]].weight().element_size())\n",
    "\n",
    "\n",
    "\"\"\"Check\"\"\"\n",
    "print(m[1][0].weight().element_size()) # 1 byte instead of 4 bytes for FP32\n",
    "\n",
    "from torch.quantization import quantize_fx\n",
    "\n",
    "\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conv.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\DeepLearning\\code\\int8测试.ipynb 单元格 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepLearning/code/int8%E6%B5%8B%E8%AF%95.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39msave(m\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39m./state_dict.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DeepLearning/code/int8%E6%B5%8B%E8%AF%95.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_int8\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./state_dict.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DeepLearning/code/int8%E6%B5%8B%E8%AF%95.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(model_int8)\n",
      "File \u001b[1;32md:\\Application\\Anaconda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:2027\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2020\u001b[0m         out \u001b[39m=\u001b[39m hook(module, incompatible_keys)\n\u001b[0;32m   2021\u001b[0m         \u001b[39massert\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, (\n\u001b[0;32m   2022\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2023\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2024\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mit should be done inplace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2025\u001b[0m         )\n\u001b[1;32m-> 2027\u001b[0m load(\u001b[39mself\u001b[39;49m, state_dict)\n\u001b[0;32m   2028\u001b[0m \u001b[39mdel\u001b[39;00m load\n\u001b[0;32m   2030\u001b[0m \u001b[39mif\u001b[39;00m strict:\n",
      "File \u001b[1;32md:\\Application\\Anaconda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:2015\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2013\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2014\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 2015\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[0;32m   2017\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m incompatible_keys \u001b[39m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[1;32md:\\Application\\Anaconda\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:2009\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(module, local_state_dict, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   2008\u001b[0m     local_metadata \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m metadata\u001b[39m.\u001b[39mget(prefix[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], {})\n\u001b[1;32m-> 2009\u001b[0m     module\u001b[39m.\u001b[39;49m_load_from_state_dict(\n\u001b[0;32m   2010\u001b[0m         local_state_dict, prefix, local_metadata, \u001b[39mTrue\u001b[39;49;00m, missing_keys, unexpected_keys, error_msgs)\n\u001b[0;32m   2011\u001b[0m     \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   2012\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Application\\Anaconda\\envs\\AI\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:157\u001b[0m, in \u001b[0;36m_ConvNd._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_state_dict\u001b[39m(\u001b[39mself\u001b[39m, state_dict, prefix, local_metadata, strict,\n\u001b[0;32m    155\u001b[0m                           missing_keys, unexpected_keys, error_msgs):\n\u001b[0;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_weight_bias(\n\u001b[1;32m--> 157\u001b[0m         state_dict[prefix \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m'\u001b[39;49m], state_dict[prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    158\u001b[0m     state_dict\u001b[39m.\u001b[39mpop(prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    159\u001b[0m     state_dict\u001b[39m.\u001b[39mpop(prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'conv.weight'"
     ]
    }
   ],
   "source": [
    "torch.save(m.state_dict(), \"./state_dict.pth\")\n",
    "model_int8.load_state_dict(torch.load(\"./state_dict.pth\"))\n",
    "print(model_int8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
